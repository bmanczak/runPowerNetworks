env_config: &env_config_train
  env_name: rte_case14_realistic_train
  keep_actions: [change_bus]
  keep_observations : [rho, gen_p, load_p, p_or, p_ex, timestep_overflow, maintenance , topo_vect]
  convert_to_tuple: True # ignored if act_on_singe or medha_actions
  act_on_single_substation: True # ignored if medha = True
  medha_actions: True
  use_parametric: False 
  rho_threshold: 0.95
  scale: True
  run_until_threshold: True # not implemented yet
  reward_scaling_factor: 3 #!choice [0.5,1, 3, 5] #3
  log_reward: False
  disable_line: -1 #!grid_search [7,8,9, 11,14,15,17,19] # check different disabled lines
  substation_actions: True
  greedy_agent : True
  conn_matrix: False

env_config: &env_config_val
  env_name: rte_case14_realistic_val
  keep_actions: [change_bus]
  keep_observations : [rho, gen_p, load_p, p_or, p_ex, timestep_overflow, maintenance , topo_vect]
  convert_to_tuple: True # ignored if act_on_singe or medha_actions
  act_on_single_substation: True # ignored if medha = True
  medha_actions: True
  use_parametric: False 
  rho_threshold: 0.95
  scale: True
  run_until_threshold: True # not implemented yet
  reward_scaling_factor: 3 #!choice [0.5,1,2,5]
  log_reward: False
  disable_line: -1 #!grid_search [7,8,9, 11,14,15,17,19] # check different disabled lines
  substation_actions: True
  greedy_agent : True
  conn_matrix: False

hidden_dim_substation_model_model: &hidden_size 256 #!choice [128,256,512]
Q_model: &Q_model
  fcnet_hiddens: [*hidden_size, *hidden_size, *hidden_size]
        
policy_model: &policy_model
  fcnet_hiddens: [*hidden_size, *hidden_size, *hidden_size]
  
tune_config:
  env: Grid_Gym_Greedy
  env_config: *env_config_train  # config to pass to env class
  log_level: INFO
  framework: torch
  seed : !choice [21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50]
  train_batch_size: 512 #!choice [256, 512]
  Q_model: *Q_model
  policy_model: *policy_model
  lr: 0.0001 #!choice [0.0001, 0.001]
  prioritized_replay: True #!choice [True, False] # default False
  optimization: 
        actor_learning_rate: 0.0001 #!choice [0.0001, 0.001, 0.00001] #0.003 
        critic_learning_rate: 0.0001 #!choice [0.0001, 0.001, 0.00001]
        entropy_learning_rate: 0.00001 #!choice [0.00001, 0.00000000000000000001] # hack to keep entropy low throught training
  learning_starts: 4096
  #target_entropy: !choice [2.038, 4.57, 1, 3]
  tau: 0.0005 #!choice [0.005, 0.0005]
  timesteps_per_iteration: 1000
  target_network_update_freq: 10 #!choice [1, 10, 100]
  rollout_fragment_length: 1
  initial_alpha: 1 #!choice [1, 1.16] #!choice [1.105, 1.16]  # log_alpha = 0.1, 0.2, 0.3, 0.5
  num_workers : 4 #!choice [2,4,6] #!choice [2,6,10] 
  callbacks : LogDistributionsCallback
  evaluation_interval: 10
  evaluation_num_episodes : 100
  evaluation_config: 
    env: Grid_Gym
    env_config: *env_config_val # use the validation env

  