env_config: &env_config
  env_name: rte_case14_realistic
  keep_actions: [change_bus]
  keep_observations : [rho, gen_p, load_p, p_or, p_ex, timestep_overflow, maintenance , topo_vect]
  convert_to_tuple: True # ignored if act_on_singe or medha_actions
  act_on_single_substation: True # ignored if medha = True
  medha_actions: True
  use_parametric: False 
  rho_threshold: 0.9
  scale: True
  run_until_threshold: True 
<<<<<<< HEAD
  reward_scaling_factor: 5
  log_reward: False
  disable_line: !grid_search [8,15,17,19] # check different disabled lines. 7,9, 14 do not work 
=======
  reward_scaling_factor: 5 #!choice [5,10,20]
  log_reward: False
  disable_line: 15 #!grid_search [7,8,9, 11,14,15,17,19] # check different disabled lines
>>>>>>> a56cea4496232dd32b8b15a8616f9108da1fdfd0

tune_config:
  env: Grid_Gym
  env_config: *env_config  # config to pass to env class
  log_level: WARN
  framework: torch
<<<<<<< HEAD
  seed : !grid_search [0,1,2,3]
=======
  seed : 420
>>>>>>> a56cea4496232dd32b8b15a8616f9108da1fdfd0
  train_batch_size: 256 #256
  twin_q: True # defualt True
  prioritized_replay: True # default False
  lr: 0.0005
  optimization: 
<<<<<<< HEAD
        actor_learning_rate: 0.0005 
        critic_learning_rate: 0.0005
        entropy_learning_rate: 0.0005
  learning_starts: 2048 # 1000
  num_workers : 9 #8
=======
        actor_learning_rate: 0.0005 #0.001 
        critic_learning_rate: 0.0005 #0.001
        entropy_learning_rate: 0.0001 #0.0001
  grad_clip: 10
  learning_starts: 2048 # 1000
  num_workers : 6 #10 #8
>>>>>>> a56cea4496232dd32b8b15a8616f9108da1fdfd0
  callbacks : LogDistributionsCallback