{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The step-wise GNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the needed libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from grid2op_env.grid_to_gym import Grid_Gym\n",
    "from evaluation.restore_agent import restore_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalize the environment. For convenience we take the `env_config` from one of the already trained SAC agents. Only modification is `conn_matrix` being `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBSTATION NUMBER: 0\n",
      "Choosing 2 out of 3\n",
      "Choosing 3 out of 3\n",
      "SUBSTATION NUMBER: 1\n",
      "Choosing 3 out of 6\n",
      "Choosing 4 out of 6\n",
      "Choosing 5 out of 6\n",
      "Choosing 6 out of 6\n",
      "SUBSTATION NUMBER: 2\n",
      "Choosing 2 out of 4\n",
      "Choosing 3 out of 4\n",
      "Choosing 4 out of 4\n",
      "SUBSTATION NUMBER: 3\n",
      "Choosing 3 out of 6\n",
      "Choosing 4 out of 6\n",
      "Choosing 5 out of 6\n",
      "Choosing 6 out of 6\n",
      "SUBSTATION NUMBER: 4\n",
      "Choosing 3 out of 5\n",
      "Choosing 4 out of 5\n",
      "Choosing 5 out of 5\n",
      "SUBSTATION NUMBER: 5\n",
      "Choosing 3 out of 6\n",
      "Choosing 4 out of 6\n",
      "Choosing 5 out of 6\n",
      "Choosing 6 out of 6\n",
      "SUBSTATION NUMBER: 6\n",
      "Choosing 2 out of 3\n",
      "Choosing 3 out of 3\n",
      "SUBSTATION NUMBER: 7\n",
      "Choosing 1 out of 2\n",
      "Choosing 2 out of 2\n",
      "SUBSTATION NUMBER: 8\n",
      "Choosing 3 out of 5\n",
      "Choosing 4 out of 5\n",
      "Choosing 5 out of 5\n",
      "SUBSTATION NUMBER: 9\n",
      "Choosing 2 out of 3\n",
      "Choosing 3 out of 3\n",
      "SUBSTATION NUMBER: 10\n",
      "Choosing 2 out of 3\n",
      "Choosing 3 out of 3\n",
      "SUBSTATION NUMBER: 11\n",
      "Choosing 2 out of 3\n",
      "Choosing 3 out of 3\n",
      "SUBSTATION NUMBER: 12\n",
      "Choosing 2 out of 4\n",
      "Choosing 3 out of 4\n",
      "Choosing 4 out of 4\n",
      "SUBSTATION NUMBER: 13\n",
      "Choosing 2 out of 3\n",
      "Choosing 3 out of 3\n",
      "all_actions dict:  {'loads_id': [(10, 1)], 'generators_id': [], 'lines_or_id': [], 'lines_ex_id': [(11, 1), (14, 1)]}\n",
      "len all actions 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/rlib_grid/lib/python3.7/site-packages/lightsim2grid/_aux_add_trafo.py:33: UserWarning: There were some Nan in the pp_net.trafo[\"tap_neutral\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_neutral\\\"], they have been replaced by 0\")\n",
      "/opt/miniconda3/envs/rlib_grid/lib/python3.7/site-packages/lightsim2grid/_aux_add_trafo.py:41: UserWarning: There were some Nan in the pp_net.trafo[\"tap_step_percent\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_step_percent\\\"], they have been replaced by 0\")\n",
      "/opt/miniconda3/envs/rlib_grid/lib/python3.7/site-packages/lightsim2grid/_aux_add_trafo.py:46: UserWarning: There were some Nan in the pp_net.trafo[\"tap_pos\"], they have been replaced by 0\n",
      "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_pos\\\"], they have been replaced by 0\")\n",
      "/opt/miniconda3/envs/rlib_grid/lib/python3.7/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n",
      "Exception ignored in: <function GymEnv.__del__ at 0x7ff0ac179170>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/rlib_grid/lib/python3.7/site-packages/grid2op/gym_compat/gymenv.py\", line 81, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/miniconda3/envs/rlib_grid/lib/python3.7/site-packages/grid2op/gym_compat/gymenv.py\", line 72, in close\n",
      "    self.action_space.close()\n",
      "AttributeError: 'CustomDiscreteActions' object has no attribute 'close'\n"
     ]
    }
   ],
   "source": [
    "env_config = {'act_on_single_substation': True,\n",
    "            'convert_to_tuple': True,\n",
    "            'env_name': 'rte_case14_realistic',\n",
    "            'keep_actions': \n",
    "                            ['change_bus'],\n",
    "            'keep_observations': ['rho',\n",
    "                                'gen_p',\n",
    "                                'load_p',\n",
    "                                'p_or',\n",
    "                                'p_ex',\n",
    "                                'timestep_overflow',\n",
    "                                'maintenance',\n",
    "                                'topo_vect'],\n",
    "            'log_reward': False,\n",
    "            'medha_actions': True,\n",
    "            'reward_scaling_factor': 3,\n",
    "            'rho_threshold': 0.9,\n",
    "            'run_until_threshold': True,\n",
    "            'scale': True,\n",
    "            'use_parametric': False,\n",
    "            'conn_matrix': True\n",
    "            }\n",
    "\n",
    "rllib_env = Grid_Gym(env_config);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the environment setup, we need to create *observation converter* that will supply the input to the neural network in a good format. The input consists of:\n",
    "\n",
    "- node features\n",
    "- the adjacenecy matrix (ices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rllib_env.org_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_obs(obs, env, hazard_threshold = 0.9):\n",
    "   \"\"\"\n",
    "   Vectorize the gym observation.\n",
    "\n",
    "   :param OrderedDict obs: gym observation\n",
    "   :param Grid2Op_environment env: grid2op environment. Used to fetch the \n",
    "      ids of different objects.\n",
    "   :param float hazard_threshold\n",
    "   \"\"\"\n",
    "\n",
    "   length = env.action_space.dim_topo # number of bus bars == number of nodes in the graph\n",
    "\n",
    "   # rho is symmetric for both ends of the line [56,1]\n",
    "   rho = torch.zeros(length)\n",
    "   rho[env.action_space.line_or_pos_topo_vect] = torch.from_numpy(obs[\"rho\"])\n",
    "   rho[env.action_space.line_ex_pos_topo_vect] = torch.from_numpy(obs[\"rho\"])\n",
    "\n",
    "   # active power p [56,1]\n",
    "   p = torch.zeros(length)\n",
    "   p[env.action_space.gen_pos_topo_vect] = torch.from_numpy(obs[\"gen_p\"]) # generator active production\n",
    "   p[env.action_space.load_pos_topo_vect] = torch.from_numpy(obs[\"load_p\"]) # load active consumption\n",
    "   p[env.action_space.line_or_pos_topo_vect] = torch.from_numpy(obs[\"p_or\"]) # origin active flow\n",
    "   p[env.action_space.line_ex_pos_topo_vect] = torch.from_numpy(obs[\"p_ex\"]) # Extremity active flow\n",
    "\n",
    "   # overflow [56,1]\n",
    "   over = torch.zeros(length)\n",
    "   over[env.action_space.line_or_pos_topo_vect] = torch.from_numpy(obs[\"timestep_overflow\"]).float()\n",
    "   over[env.action_space.line_ex_pos_topo_vect] = torch.from_numpy(obs[\"timestep_overflow\"]).float()\n",
    "\n",
    "   # one-hot topo vector [56,3]\n",
    "   topo_vect_one_hot = torch.zeros(length,3)\n",
    "   topo_vect = obs[\"topo_vect\"]\n",
    "   topo_vect[topo_vect==-1] = 0 # change disconneted from -1 to 0\n",
    "   topo_vect_one_hot = torch.nn.functional.one_hot(torch.from_numpy(topo_vect).to(torch.int64), num_classes=3)\n",
    "\n",
    "   # powerline maintenance\n",
    "   # maintenance = torch.zeros(length)\n",
    "   # maintenance[env.action_space.line_or_pos_topo_vect] = torch.from_numpy(obs[\"maintenance\"]).float()\n",
    "   # maintenance[env.action_space.line_ex_pos_topo_vect] = torch.from_numpy(obs[\"maintenance\"]).float()\n",
    "\n",
    "   # manual feature thresholding \n",
    "   hazard = torch.zeros(length) # [56,1]\n",
    "   hazard[env.action_space.line_or_pos_topo_vect] = (torch.from_numpy(obs[\"rho\"]) > hazard_threshold).float()\n",
    "   hazard[env.action_space.line_ex_pos_topo_vect] = (torch.from_numpy(obs[\"rho\"]) > hazard_threshold).float()\n",
    "\n",
    "   vectorized_obs = torch.stack([rho,p,over, hazard], dim = 1)\n",
    "   vectorized_obs = torch.concat([vectorized_obs, topo_vect_one_hot], dim = -1)\n",
    "   \n",
    "   return vectorized_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 7])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_obs(rllib_env.reset(), env).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we get the feature vector of the shape `(n_nodes, n_features)`.\n",
    "What will also be useful down the road is the information on what elements belong to what substation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([ 1,  2,  3,  4,  5,  8,  9, 10, 11, 12, 13], dtype=int32),\n",
       "  array([1, 2, 5, 7, 0], dtype=int32),\n",
       "  array([ 0,  0,  1,  1,  1,  2,  3,  5,  5,  5,  8,  8,  9, 11, 12,  3,  3,\n",
       "          4,  6,  8], dtype=int32),\n",
       "  array([ 1,  4,  2,  3,  4,  3,  4, 10, 11, 12,  9, 13, 10, 12, 13,  6,  8,\n",
       "          5,  7,  6], dtype=int32),\n",
       "  array([], dtype=int32)],\n",
       " '------ \\n',\n",
       " [array([ 8, 12, 18, 23, 29, 39, 42, 45, 48, 52, 55], dtype=int32),\n",
       "  array([ 7, 11, 28, 34,  2], dtype=int32),\n",
       "  array([ 0,  1,  4,  5,  6, 10, 15, 24, 25, 26, 35, 36, 41, 47, 51, 16, 17,\n",
       "         22, 31, 38], dtype=int32),\n",
       "  array([ 3, 19,  9, 13, 20, 14, 21, 43, 46, 49, 40, 53, 44, 50, 54, 30, 37,\n",
       "         27, 33, 32], dtype=int32),\n",
       "  array([], dtype=int32)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem_to_sub_id_arrays = [env.load_to_subid, env.gen_to_subid, env.line_or_to_subid, env.line_ex_to_subid, env.storage_to_subid]\n",
    "elem_to_elem_id = [env.reset().load_pos_topo_vect, env.reset().gen_pos_topo_vect, env.reset().line_or_pos_topo_vect, env.reset().line_ex_pos_topo_vect, env.reset().storage_pos_topo_vect]\n",
    "elem_to_sub_id_arrays, \"------ \\n\", elem_to_elem_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "sub_id_to_elem_id = defaultdict(list) # necessary for pooling over substations\n",
    "\n",
    "for sub_id_arr, elem_id_arr in zip(elem_to_sub_id_arrays,elem_to_elem_id):\n",
    "    for sub_id, elem_id in zip(sub_id_arr, elem_id_arr):\n",
    "        sub_id_to_elem_id[sub_id].append(elem_id)\n",
    "\n",
    "sub_id_to_elem_id = OrderedDict(sorted(sub_id_to_elem_id.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if we have correctly assigned each element to a correct substation we can look at the global `env.sub_info` information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_id, num_connected in enumerate(env.sub_info):\n",
    "    if len(sub_id_to_elem_id[sub_id]) != num_connected:\n",
    "        raise ValueError(\"Something went wrong for substation {sub_id}\".format(sub_id = sub_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rllib_env.all_actions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder \n",
    "Now we can proceed to programming the encoder.\n",
    "The encoder will have the following structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from layers.graph_attention_layer import GATLayer\n",
    "\n",
    "def pool_per_substation(arr, sub_to_id, pooling_operator = \"mean\"):\n",
    "    \"\"\"\n",
    "    Pool the observations over the substations.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    ----------\n",
    "    arr: np.array\n",
    "        The array to pool over\n",
    "    dim: int\n",
    "        The dimension to pool over.\n",
    "    sub_to_id: dict\n",
    "        The dictionary that maps the substation id to the index of the array.\n",
    "    pooling_operator: str\n",
    "        \"mean\" or \"max\". Defaults to mean.\n",
    "    \"\"\"\n",
    "\n",
    "    pooled = [] \n",
    "    for sub, elements in sub_to_id.items():\n",
    "        if pooling_operator == \"max\":\n",
    "            pooled.append(torch.mean(arr[:, elements, :], dim = 1, keepdim=True))\n",
    "        else:\n",
    "            pooled.append(torch.mean(arr[:, elements, :], dim = 1, keepdim=True))\n",
    "    \n",
    "    return torch.cat(pooled, dim = 1)\n",
    "        \n",
    "\n",
    "class SubstationModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, nheads, num_layers = 6, dropout=0):\n",
    "        \"\"\"\n",
    "        Constructs the Encoder.\n",
    "\n",
    "        Args:\n",
    "            num_features (int): Number of features of each node.\n",
    "            hidden_dim ([int]): Number of features of each node after transformation.\n",
    "            nheads (int): number of attention heads.\n",
    "            num_layers (int, optional): Number of GAT layers. Defaults to 6.\n",
    "            dropout (int, optional): Dropout probability. Defaults to 0.\n",
    "        \"\"\"\n",
    "        super(SubstationModel, self).__init__()\n",
    "        self.linear = nn.Linear(num_features, hidden_dim)\n",
    "        self.gat_layers = nn.ModuleList([\n",
    "                GATLayer(hidden_dim, nheads, dropout=dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.classify_substations = nn.Linear(hidden_dim, 1) # a binary classifier on each substation\n",
    "        \n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        print(x.shape)\n",
    "        x = self.linear(x)\n",
    "        for layer in self.gat_layers:\n",
    "            node_embeddings = layer(x, adj)\n",
    "        substation_embeddings = pool_per_substation(node_embeddings, sub_id_to_elem_id)\n",
    "        \n",
    "        substation_prob_distr = F.softmax(self.classify_substations(substation_embeddings), dim = 1)\n",
    "\n",
    "        return substation_prob_distr, node_embeddings, substation_embeddings\n",
    "\n",
    "class NodeModelNodeLevel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, nheads, num_layers = 2, dropout=0):\n",
    "        \"\"\"\n",
    "        Predicts on which bar the node should be changed\n",
    "\n",
    "        Args:\n",
    "            num_features (int): Number of features of each node.\n",
    "            hidden_dim ([int]): Number of features of each node after transformation.\n",
    "            nheads (int): number of attention heads.\n",
    "            num_layers (int, optional): Number of GAT layers. Defaults to 6.\n",
    "            dropout (int, optional): Dropout probability. Defaults to 0.\n",
    "        \"\"\"\n",
    "        super(NodeModelNodeLevel, self).__init__()\n",
    "\n",
    "\n",
    "        self.gat_layers = nn.ModuleList([\n",
    "                GATLayer(hidden_dim, nheads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.classify_nodes = nn.Linear(hidden_dim, 1) # a binary classifier on each node\n",
    "\n",
    "    \n",
    "    def forward(self, x, adj = None):\n",
    "\n",
    "        if adj is None:\n",
    "              adj = torch.ones(size = (x.shape[0], x.shape[1], x.shape[1])) # fully_connecnted_adj, pays attention to padding (wrong)\n",
    "\n",
    "        for layer in self.gat_layers:\n",
    "            node_embeddings = layer(x,adj)\n",
    "\n",
    "        # return node_embeddings\n",
    "        node_prob_distr = F.softmax(self.classify_nodes(node_embeddings), dim = 1)\n",
    "        \n",
    "        return node_prob_distr\n",
    "\n",
    "\n",
    "\n",
    "class StepwiseGNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, substaion_model, node_model):\n",
    "        \"\"\"\n",
    "        Puts the Substation and Node models together.\n",
    "        \"\"\"\n",
    "        super(StepwiseGNNModel, self).__init__()\n",
    "\n",
    "        self.substaion_model = substaion_model\n",
    "        self.node_model = node_model\n",
    "\n",
    "    def forward(self,x, adj):\n",
    "        substation_prob_distr, node_embeddings, substation_embeddings = self.substaion_model(x, adj)\n",
    "\n",
    "        sampler = torch.distributions.Categorical(probs = substation_prob_distr.flatten())\n",
    "        sampled_sub = sampler.sample().item()\n",
    "        print(f\"Sampled substation {sampled_sub} with the following nodes {sub_id_to_elem_id[sampled_sub]}\")\n",
    "\n",
    "        subset_node_embeddings_idx = sub_id_to_elem_id[sampled_sub]\n",
    "        subset_node_embeddings = node_embeddings[:, subset_node_embeddings_idx, :]\n",
    "\n",
    "        substation_embeddings.shape, subset_node_embeddings.shape\n",
    "\n",
    "        context_embedding = torch.mean(substation_embeddings, dim = 1, keepdim=True).expand(subset_node_embeddings.shape)\n",
    "        enriched_node_embedding = torch.cat((context_embedding, subset_node_embeddings), dim = 2)\n",
    "\n",
    "        node_prob_distr = self.node_model(enriched_node_embedding)\n",
    "\n",
    "        return node_prob_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op_env.medha_action_space import create_action_space, remove_redundant_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeModelLinkPrediction(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NodeModelLinkPrediction. self).__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 7\n",
    "hidden_dim = 128\n",
    "nheads = 1\n",
    "num_layers_substation = 6\n",
    "num_layers_node = 3\n",
    "\n",
    "\n",
    "\n",
    "enc = SubstationModel(num_features = num_features, hidden_dim = hidden_dim, nheads = nheads)\n",
    "dec = NodeModelNodeLevel(hidden_dim = 2*hidden_dim, nheads = nheads)\n",
    "\n",
    "the_model = StepwiseGNNModel(enc, dec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 56, 7])\n",
      "after softamx tensor([[[0.2246, 0.2303, 0.3004,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2232, 0.2288, 0.2979,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3227, 0.3283, 0.3490,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1977, 0.2472, 0.3436],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1994, 0.2377, 0.3269],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2574, 0.3210, 0.4216]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[-1.2392, -0.1989,  1.5650,  ...,  0.9723,  0.1651, -1.0575],\n",
      "         [-1.2334, -0.1969,  1.5692,  ...,  0.9766,  0.1711, -1.0626],\n",
      "         [-1.2872, -0.1671,  1.6245,  ...,  0.9633,  0.2025, -1.0490],\n",
      "         ...,\n",
      "         [-1.4649, -0.2313,  1.4603,  ...,  0.8024, -0.0048, -0.8586],\n",
      "         [-1.4920, -0.2010,  1.5310,  ...,  0.8155,  0.0460, -0.8756],\n",
      "         [-1.4941, -0.1792,  1.5706,  ...,  0.8241,  0.0830, -0.8870]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[-1.2392, -0.1989,  1.5650,  ...,  0.9723,  0.1651, -1.0575],\n",
      "         [-1.2334, -0.1969,  1.5692,  ...,  0.9766,  0.1711, -1.0626],\n",
      "         [-1.2872, -0.1671,  1.6245,  ...,  0.9633,  0.2025, -1.0490],\n",
      "         ...,\n",
      "         [-1.4649, -0.2313,  1.4603,  ...,  0.8024, -0.0048, -0.8586],\n",
      "         [-1.4920, -0.2010,  1.5310,  ...,  0.8155,  0.0460, -0.8756],\n",
      "         [-1.4941, -0.1792,  1.5706,  ...,  0.8241,  0.0830, -0.8870]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[0.8829, 0.0000, 0.4359,  ..., 0.4412, 0.0000, 1.0528],\n",
      "         [0.8877, 0.0000, 0.4429,  ..., 0.4368, 0.0000, 1.0519],\n",
      "         [0.8786, 0.0000, 0.4405,  ..., 0.4616, 0.0000, 1.0830],\n",
      "         ...,\n",
      "         [0.6993, 0.0000, 0.1973,  ..., 0.6026, 0.0000, 1.0931],\n",
      "         [0.7179, 0.0000, 0.2271,  ..., 0.6068, 0.0000, 1.1222],\n",
      "         [0.7305, 0.0000, 0.2512,  ..., 0.6021, 0.0000, 1.1321]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 0.2351,  0.1797,  0.0089,  ..., -0.1566, -0.1138,  0.1934],\n",
      "         [ 0.2289,  0.1755,  0.0047,  ..., -0.1626, -0.1204,  0.1969],\n",
      "         [ 0.2782,  0.1114,  0.0738,  ..., -0.1025, -0.2546,  0.1423],\n",
      "         ...,\n",
      "         [ 0.3354,  0.2471,  0.0646,  ..., -0.0631, -0.0087,  0.1441],\n",
      "         [ 0.3309,  0.1983,  0.0796,  ..., -0.0614, -0.1024,  0.1347],\n",
      "         [ 0.1970,  0.1633, -0.0362,  ..., -0.1988, -0.1399,  0.2330]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([1, 56, 128])\n",
      "chuuj tensor([[[ 0.2351,  0.1797,  0.0089,  ..., -0.1566, -0.1138,  0.1934],\n",
      "         [ 0.2289,  0.1755,  0.0047,  ..., -0.1626, -0.1204,  0.1969],\n",
      "         [ 0.2782,  0.1114,  0.0738,  ..., -0.1025, -0.2546,  0.1423],\n",
      "         ...,\n",
      "         [ 0.3354,  0.2471,  0.0646,  ..., -0.0631, -0.0087,  0.1441],\n",
      "         [ 0.3309,  0.1983,  0.0796,  ..., -0.0614, -0.1024,  0.1347],\n",
      "         [ 0.1970,  0.1633, -0.0362,  ..., -0.1988, -0.1399,  0.2330]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after softamx tensor([[[0.2904, 0.2948, 0.2033,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2884, 0.2931, 0.2052,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3556, 0.3627, 0.2816,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2817, 0.2286, 0.2947],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2539, 0.2162, 0.2978],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2673, 0.2675, 0.4652]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[ 0.8036,  1.0685,  0.2949,  ..., -0.6656, -0.8842, -0.3767],\n",
      "         [ 0.7994,  1.0620,  0.2875,  ..., -0.6690, -0.8796, -0.3758],\n",
      "         [ 0.8099,  1.0726,  0.2888,  ..., -0.6691, -0.8662, -0.3830],\n",
      "         ...,\n",
      "         [ 0.9243,  1.2636,  0.5392,  ..., -0.5328, -1.0268, -0.3960],\n",
      "         [ 0.9271,  1.2586,  0.5127,  ..., -0.5528, -0.9965, -0.4049],\n",
      "         [ 0.9125,  1.2274,  0.4606,  ..., -0.5794, -0.9448, -0.4092]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[ 0.8036,  1.0685,  0.2949,  ..., -0.6656, -0.8842, -0.3767],\n",
      "         [ 0.7994,  1.0620,  0.2875,  ..., -0.6690, -0.8796, -0.3758],\n",
      "         [ 0.8099,  1.0726,  0.2888,  ..., -0.6691, -0.8662, -0.3830],\n",
      "         ...,\n",
      "         [ 0.9243,  1.2636,  0.5392,  ..., -0.5328, -1.0268, -0.3960],\n",
      "         [ 0.9271,  1.2586,  0.5127,  ..., -0.5528, -0.9965, -0.4049],\n",
      "         [ 0.9125,  1.2274,  0.4606,  ..., -0.5794, -0.9448, -0.4092]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.6113],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.6154],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.6225],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.4501],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.4823],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.5244]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 0.2607,  0.1436,  0.0687,  ..., -0.0174, -0.1213,  0.2483],\n",
      "         [ 0.2542,  0.1394,  0.0642,  ..., -0.0224, -0.1280,  0.2523],\n",
      "         [ 0.3059,  0.0734,  0.1232,  ...,  0.0395, -0.2618,  0.1922],\n",
      "         ...,\n",
      "         [ 0.3555,  0.2103,  0.1314,  ...,  0.0502, -0.0156,  0.1792],\n",
      "         [ 0.3502,  0.1611,  0.1396,  ...,  0.0567, -0.1107,  0.1685],\n",
      "         [ 0.2037,  0.1290,  0.0234,  ..., -0.0745, -0.1518,  0.2696]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([1, 56, 128])\n",
      "chuuj tensor([[[ 0.2607,  0.1436,  0.0687,  ..., -0.0174, -0.1213,  0.2483],\n",
      "         [ 0.2542,  0.1394,  0.0642,  ..., -0.0224, -0.1280,  0.2523],\n",
      "         [ 0.3059,  0.0734,  0.1232,  ...,  0.0395, -0.2618,  0.1922],\n",
      "         ...,\n",
      "         [ 0.3555,  0.2103,  0.1314,  ...,  0.0502, -0.0156,  0.1792],\n",
      "         [ 0.3502,  0.1611,  0.1396,  ...,  0.0567, -0.1107,  0.1685],\n",
      "         [ 0.2037,  0.1290,  0.0234,  ..., -0.0745, -0.1518,  0.2696]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after softamx tensor([[[0.2605, 0.2568, 0.2271,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2606, 0.2570, 0.2293,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3571, 0.3500, 0.2930,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2804, 0.2484, 0.2121],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2868, 0.2517, 0.2060],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.3635, 0.3486, 0.2878]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[-0.5896,  0.3976,  0.6536,  ...,  0.3043,  0.7510, -0.2611],\n",
      "         [-0.5896,  0.3936,  0.6570,  ...,  0.3185,  0.7578, -0.2560],\n",
      "         [-0.6092,  0.4086,  0.7024,  ...,  0.3214,  0.7204, -0.2546],\n",
      "         ...,\n",
      "         [-0.5785,  0.5359,  0.5294,  ..., -0.2133,  0.4810, -0.4369],\n",
      "         [-0.6046,  0.5360,  0.5972,  ..., -0.1413,  0.4798, -0.4146],\n",
      "         [-0.6116,  0.5333,  0.6223,  ..., -0.1108,  0.4775, -0.4026]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[-0.5896,  0.3976,  0.6536,  ...,  0.3043,  0.7510, -0.2611],\n",
      "         [-0.5896,  0.3936,  0.6570,  ...,  0.3185,  0.7578, -0.2560],\n",
      "         [-0.6092,  0.4086,  0.7024,  ...,  0.3214,  0.7204, -0.2546],\n",
      "         ...,\n",
      "         [-0.5785,  0.5359,  0.5294,  ..., -0.2133,  0.4810, -0.4369],\n",
      "         [-0.6046,  0.5360,  0.5972,  ..., -0.1413,  0.4798, -0.4146],\n",
      "         [-0.6116,  0.5333,  0.6223,  ..., -0.1108,  0.4775, -0.4026]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[0.6268, 0.0000, 0.0000,  ..., 0.0000, 0.4864, 0.6329],\n",
      "         [0.6225, 0.0000, 0.0000,  ..., 0.0000, 0.4885, 0.6342],\n",
      "         [0.6058, 0.0000, 0.0000,  ..., 0.0000, 0.5267, 0.6322],\n",
      "         ...,\n",
      "         [0.7634, 0.0000, 0.0000,  ..., 0.0000, 0.4161, 0.5713],\n",
      "         [0.7315, 0.0000, 0.0000,  ..., 0.0000, 0.4676, 0.5806],\n",
      "         [0.7145, 0.0000, 0.0000,  ..., 0.0000, 0.4873, 0.5815]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 0.2401,  0.1656, -0.0053,  ..., -0.0172, -0.1152,  0.1860],\n",
      "         [ 0.2341,  0.1619, -0.0101,  ..., -0.0218, -0.1217,  0.1901],\n",
      "         [ 0.2717,  0.0952,  0.0459,  ...,  0.0041, -0.2541,  0.1377],\n",
      "         ...,\n",
      "         [ 0.3264,  0.2195,  0.0623,  ...,  0.0421, -0.0100,  0.1367],\n",
      "         [ 0.3150,  0.1722,  0.0669,  ...,  0.0315, -0.1019,  0.1312],\n",
      "         [ 0.1824,  0.1449, -0.0538,  ..., -0.0731, -0.1365,  0.2417]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([1, 56, 128])\n",
      "chuuj tensor([[[ 0.2401,  0.1656, -0.0053,  ..., -0.0172, -0.1152,  0.1860],\n",
      "         [ 0.2341,  0.1619, -0.0101,  ..., -0.0218, -0.1217,  0.1901],\n",
      "         [ 0.2717,  0.0952,  0.0459,  ...,  0.0041, -0.2541,  0.1377],\n",
      "         ...,\n",
      "         [ 0.3264,  0.2195,  0.0623,  ...,  0.0421, -0.0100,  0.1367],\n",
      "         [ 0.3150,  0.1722,  0.0669,  ...,  0.0315, -0.1019,  0.1312],\n",
      "         [ 0.1824,  0.1449, -0.0538,  ..., -0.0731, -0.1365,  0.2417]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after softamx tensor([[[0.2813, 0.2852, 0.2173,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2803, 0.2843, 0.2176,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3566, 0.3621, 0.2813,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2603, 0.2273, 0.3129],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2457, 0.2164, 0.3089],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.3009, 0.2756, 0.4235]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[-0.3061, -0.9404, -0.4370,  ..., -0.0394,  0.5847,  0.0475],\n",
      "         [-0.3042, -0.9379, -0.4327,  ..., -0.0323,  0.5789,  0.0456],\n",
      "         [-0.3289, -0.9445, -0.4092,  ..., -0.0487,  0.6236,  0.0301],\n",
      "         ...,\n",
      "         [-0.3774, -1.0123, -0.5715,  ..., -0.2977,  0.7988,  0.1071],\n",
      "         [-0.3970, -1.0147, -0.5303,  ..., -0.2827,  0.8231,  0.0824],\n",
      "         [-0.4078, -1.0079, -0.4912,  ..., -0.2630,  0.8319,  0.0613]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[-0.3061, -0.9404, -0.4370,  ..., -0.0394,  0.5847,  0.0475],\n",
      "         [-0.3042, -0.9379, -0.4327,  ..., -0.0323,  0.5789,  0.0456],\n",
      "         [-0.3289, -0.9445, -0.4092,  ..., -0.0487,  0.6236,  0.0301],\n",
      "         ...,\n",
      "         [-0.3774, -1.0123, -0.5715,  ..., -0.2977,  0.7988,  0.1071],\n",
      "         [-0.3970, -1.0147, -0.5303,  ..., -0.2827,  0.8231,  0.0824],\n",
      "         [-0.4078, -1.0079, -0.4912,  ..., -0.2630,  0.8319,  0.0613]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[0.3189, 0.0000, 0.2861,  ..., 1.6571, 0.7882, 0.8279],\n",
      "         [0.3189, 0.0000, 0.2842,  ..., 1.6553, 0.7872, 0.8261],\n",
      "         [0.2863, 0.0000, 0.2772,  ..., 1.6175, 0.7873, 0.8436],\n",
      "         ...,\n",
      "         [0.2965, 0.0000, 0.3451,  ..., 1.6687, 0.8063, 0.8856],\n",
      "         [0.2654, 0.0000, 0.3314,  ..., 1.6333, 0.8081, 0.9007],\n",
      "         [0.2389, 0.0000, 0.3166,  ..., 1.5940, 0.8030, 0.9059]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 0.3337,  0.1421,  0.0750,  ..., -0.0855, -0.0943,  0.2159],\n",
      "         [ 0.3276,  0.1382,  0.0700,  ..., -0.0918, -0.1007,  0.2197],\n",
      "         [ 0.3751,  0.0750,  0.1281,  ..., -0.0311, -0.2375,  0.1655],\n",
      "         ...,\n",
      "         [ 0.4219,  0.2027,  0.1470,  ...,  0.0042,  0.0006,  0.1660],\n",
      "         [ 0.4184,  0.1559,  0.1517,  ...,  0.0053, -0.0903,  0.1571],\n",
      "         [ 0.2840,  0.1239,  0.0291,  ..., -0.1410, -0.1235,  0.2587]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([1, 56, 128])\n",
      "chuuj tensor([[[ 0.3337,  0.1421,  0.0750,  ..., -0.0855, -0.0943,  0.2159],\n",
      "         [ 0.3276,  0.1382,  0.0700,  ..., -0.0918, -0.1007,  0.2197],\n",
      "         [ 0.3751,  0.0750,  0.1281,  ..., -0.0311, -0.2375,  0.1655],\n",
      "         ...,\n",
      "         [ 0.4219,  0.2027,  0.1470,  ...,  0.0042,  0.0006,  0.1660],\n",
      "         [ 0.4184,  0.1559,  0.1517,  ...,  0.0053, -0.0903,  0.1571],\n",
      "         [ 0.2840,  0.1239,  0.0291,  ..., -0.1410, -0.1235,  0.2587]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after softamx tensor([[[0.2408, 0.2306, 0.2346,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2435, 0.2331, 0.2376,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3443, 0.3286, 0.3270,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2828, 0.2507, 0.1278],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.3183, 0.2792, 0.1338],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.4394, 0.3865, 0.1741]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[-1.6869,  1.5237,  0.6816,  ...,  1.0022, -0.6337, -1.4743],\n",
      "         [-1.6920,  1.5300,  0.6900,  ...,  0.9922, -0.6351, -1.4682],\n",
      "         [-1.7228,  1.5400,  0.7588,  ...,  1.0023, -0.6683, -1.4274],\n",
      "         ...,\n",
      "         [-1.4789,  1.2607,  0.4056,  ...,  1.3563, -0.5898, -1.6274],\n",
      "         [-1.5614,  1.3199,  0.5437,  ...,  1.3327, -0.6476, -1.5619],\n",
      "         [-1.5609,  1.3132,  0.5537,  ...,  1.3406, -0.6536, -1.5509]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[-1.6869,  1.5237,  0.6816,  ...,  1.0022, -0.6337, -1.4743],\n",
      "         [-1.6920,  1.5300,  0.6900,  ...,  0.9922, -0.6351, -1.4682],\n",
      "         [-1.7228,  1.5400,  0.7588,  ...,  1.0023, -0.6683, -1.4274],\n",
      "         ...,\n",
      "         [-1.4789,  1.2607,  0.4056,  ...,  1.3563, -0.5898, -1.6274],\n",
      "         [-1.5614,  1.3199,  0.5437,  ...,  1.3327, -0.6476, -1.5619],\n",
      "         [-1.5609,  1.3132,  0.5537,  ...,  1.3406, -0.6536, -1.5509]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[1.2442, 0.0000, 1.1305,  ..., 0.0000, 0.4411, 0.0000],\n",
      "         [1.2512, 0.0000, 1.1341,  ..., 0.0000, 0.4367, 0.0000],\n",
      "         [1.2513, 0.0000, 1.1439,  ..., 0.0000, 0.4412, 0.0000],\n",
      "         ...,\n",
      "         [0.9529, 0.0000, 0.9769,  ..., 0.1450, 0.5967, 0.0000],\n",
      "         [0.9995, 0.0000, 1.0180,  ..., 0.1322, 0.5873, 0.0000],\n",
      "         [0.9903, 0.0000, 1.0148,  ..., 0.1386, 0.5905, 0.0000]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 0.1430,  0.0592,  0.0289,  ..., -0.0636, -0.0921,  0.2423],\n",
      "         [ 0.1381,  0.0551,  0.0237,  ..., -0.0692, -0.0984,  0.2460],\n",
      "         [ 0.1898, -0.0032,  0.0893,  ...,  0.0008, -0.2306,  0.1897],\n",
      "         ...,\n",
      "         [ 0.2153,  0.1249,  0.1024,  ...,  0.0166,  0.0085,  0.1907],\n",
      "         [ 0.2158,  0.0809,  0.1109,  ...,  0.0258, -0.0833,  0.1819],\n",
      "         [ 0.0965,  0.0467, -0.0189,  ..., -0.1173, -0.1155,  0.2842]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([1, 56, 128])\n",
      "chuuj tensor([[[ 0.1430,  0.0592,  0.0289,  ..., -0.0636, -0.0921,  0.2423],\n",
      "         [ 0.1381,  0.0551,  0.0237,  ..., -0.0692, -0.0984,  0.2460],\n",
      "         [ 0.1898, -0.0032,  0.0893,  ...,  0.0008, -0.2306,  0.1897],\n",
      "         ...,\n",
      "         [ 0.2153,  0.1249,  0.1024,  ...,  0.0166,  0.0085,  0.1907],\n",
      "         [ 0.2158,  0.0809,  0.1109,  ...,  0.0258, -0.0833,  0.1819],\n",
      "         [ 0.0965,  0.0467, -0.0189,  ..., -0.1173, -0.1155,  0.2842]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after softamx tensor([[[0.1912, 0.1839, 0.2984,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1934, 0.1859, 0.3007,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2970, 0.2826, 0.4203,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2264, 0.2758, 0.1176],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2739, 0.3187, 0.1220],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.3879, 0.4459, 0.1662]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[-2.9265,  1.1768,  1.8637,  ...,  0.7882,  1.6036, -0.0562],\n",
      "         [-2.9188,  1.1758,  1.8638,  ...,  0.7800,  1.6057, -0.0405],\n",
      "         [-2.9260,  1.1975,  1.8598,  ...,  0.7715,  1.5814, -0.0618],\n",
      "         ...,\n",
      "         [-3.1403,  1.2052,  1.8122,  ...,  1.0568,  1.4644, -0.6457],\n",
      "         [-3.1594,  1.2500,  1.8293,  ...,  1.0216,  1.4553, -0.6186],\n",
      "         [-3.1578,  1.2531,  1.8233,  ...,  1.0222,  1.4433, -0.6333]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[-2.9265,  1.1768,  1.8637,  ...,  0.7882,  1.6036, -0.0562],\n",
      "         [-2.9188,  1.1758,  1.8638,  ...,  0.7800,  1.6057, -0.0405],\n",
      "         [-2.9260,  1.1975,  1.8598,  ...,  0.7715,  1.5814, -0.0618],\n",
      "         ...,\n",
      "         [-3.1403,  1.2052,  1.8122,  ...,  1.0568,  1.4644, -0.6457],\n",
      "         [-3.1594,  1.2500,  1.8293,  ...,  1.0216,  1.4553, -0.6186],\n",
      "         [-3.1578,  1.2531,  1.8233,  ...,  1.0222,  1.4433, -0.6333]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[0.0000, 0.0000, 0.0000,  ..., 1.7475, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.7443, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.7188, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.8026, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.7707, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.7617, 0.0000, 0.0000]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 0.2496,  0.1343,  0.0466,  ..., -0.1525, -0.0230,  0.2596],\n",
      "         [ 0.2434,  0.1309,  0.0419,  ..., -0.1587, -0.0294,  0.2639],\n",
      "         [ 0.2906,  0.0620,  0.1028,  ..., -0.0941, -0.1559,  0.2071],\n",
      "         ...,\n",
      "         [ 0.3415,  0.1813,  0.1131,  ..., -0.0644,  0.0697,  0.1999],\n",
      "         [ 0.3353,  0.1326,  0.1220,  ..., -0.0626, -0.0158,  0.1923],\n",
      "         [ 0.1962,  0.1065,  0.0020,  ..., -0.2115, -0.0563,  0.3034]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([1, 56, 128])\n",
      "chuuj tensor([[[ 0.2496,  0.1343,  0.0466,  ..., -0.1525, -0.0230,  0.2596],\n",
      "         [ 0.2434,  0.1309,  0.0419,  ..., -0.1587, -0.0294,  0.2639],\n",
      "         [ 0.2906,  0.0620,  0.1028,  ..., -0.0941, -0.1559,  0.2071],\n",
      "         ...,\n",
      "         [ 0.3415,  0.1813,  0.1131,  ..., -0.0644,  0.0697,  0.1999],\n",
      "         [ 0.3353,  0.1326,  0.1220,  ..., -0.0626, -0.0158,  0.1923],\n",
      "         [ 0.1962,  0.1065,  0.0020,  ..., -0.2115, -0.0563,  0.3034]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "obs = rllib_env.reset()\n",
    "substation_prob_distr, node_embeddings, substation_embeddings = enc(vectorize_obs(obs, env).unsqueeze(0), torch.from_numpy(obs[\"connectivity_matrix\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting encoder and decoder together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = torch.distributions.Categorical(probs = substation_prob_distr.flatten())\n",
    "sampled_sub = sampler.sample().item()\n",
    "\n",
    "subset_node_embeddings_idx = sub_id_to_elem_id[sampled_sub]\n",
    "subset_node_embeddings = node_embeddings[:, subset_node_embeddings_idx, :]\n",
    "\n",
    "substation_embeddings.shape, subset_node_embeddings.shape\n",
    "\n",
    "context_embedding = torch.mean(substation_embeddings, dim = 1, keepdim=True).expand(subset_node_embeddings.shape)\n",
    "enriched_node_embedding = torch.cat((context_embedding, subset_node_embeddings), dim = 2)\n",
    "\n",
    "enriched_node_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0718],\n",
       "         [0.0718],\n",
       "         [0.0720],\n",
       "         [0.0717],\n",
       "         [0.0717],\n",
       "         [0.0707],\n",
       "         [0.0711],\n",
       "         [0.0718],\n",
       "         [0.0714],\n",
       "         [0.0716],\n",
       "         [0.0712],\n",
       "         [0.0715],\n",
       "         [0.0705],\n",
       "         [0.0711]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substation_prob_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_embedding.shape, subset_node_embeddings.shape\n",
    "\n",
    "context_embedding.expand(subset_node_embeddings.shape).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping the probabilities to actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 56, 7])\n",
      "after softamx tensor([[[0.2268, 0.2305, 0.2988,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2246, 0.2283, 0.2975,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3232, 0.3285, 0.3483,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1981, 0.2470, 0.3422],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1979, 0.2384, 0.3242],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2600, 0.3196, 0.4204]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[-1.2389, -0.1980,  1.5673,  ...,  0.9734,  0.1671, -1.0588],\n",
      "         [-1.2282, -0.1987,  1.5658,  ...,  0.9785,  0.1699, -1.0646],\n",
      "         [-1.2800, -0.1697,  1.6199,  ...,  0.9659,  0.2009, -1.0519],\n",
      "         ...,\n",
      "         [-1.4576, -0.2269,  1.4743,  ...,  0.8134,  0.0090, -0.8715],\n",
      "         [-1.4738, -0.2007,  1.5372,  ...,  0.8300,  0.0568, -0.8924],\n",
      "         [-1.4890, -0.1743,  1.5853,  ...,  0.8339,  0.0968, -0.8987]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[-1.2389, -0.1980,  1.5673,  ...,  0.9734,  0.1671, -1.0588],\n",
      "         [-1.2282, -0.1987,  1.5658,  ...,  0.9785,  0.1699, -1.0646],\n",
      "         [-1.2800, -0.1697,  1.6199,  ...,  0.9659,  0.2009, -1.0519],\n",
      "         ...,\n",
      "         [-1.4576, -0.2269,  1.4743,  ...,  0.8134,  0.0090, -0.8715],\n",
      "         [-1.4738, -0.2007,  1.5372,  ...,  0.8300,  0.0568, -0.8924],\n",
      "         [-1.4890, -0.1743,  1.5853,  ...,  0.8339,  0.0968, -0.8987]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[0.8842, 0.0000, 0.4377,  ..., 0.4405, 0.0000, 1.0533],\n",
      "         [0.8893, 0.0000, 0.4443,  ..., 0.4342, 0.0000, 1.0494],\n",
      "         [0.8809, 0.0000, 0.4427,  ..., 0.4579, 0.0000, 1.0796],\n",
      "         ...,\n",
      "         [0.7115, 0.0000, 0.2129,  ..., 0.5952, 0.0000, 1.0953],\n",
      "         [0.7331, 0.0000, 0.2454,  ..., 0.5939, 0.0000, 1.1189],\n",
      "         [0.7416, 0.0000, 0.2657,  ..., 0.5960, 0.0000, 1.1351]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 0.2375,  0.1773,  0.0121,  ..., -0.1537, -0.1189,  0.1908],\n",
      "         [ 0.2287,  0.1755,  0.0046,  ..., -0.1627, -0.1204,  0.1969],\n",
      "         [ 0.2787,  0.1110,  0.0746,  ..., -0.1018, -0.2555,  0.1416],\n",
      "         ...,\n",
      "         [ 0.3296,  0.2437,  0.0612,  ..., -0.0690, -0.0140,  0.1475],\n",
      "         [ 0.3155,  0.1983,  0.0661,  ..., -0.0777, -0.0988,  0.1462],\n",
      "         [ 0.2038,  0.1591, -0.0273,  ..., -0.1910, -0.1493,  0.2256]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([1, 56, 128])\n",
      "chuuj tensor([[[ 0.2375,  0.1773,  0.0121,  ..., -0.1537, -0.1189,  0.1908],\n",
      "         [ 0.2287,  0.1755,  0.0046,  ..., -0.1627, -0.1204,  0.1969],\n",
      "         [ 0.2787,  0.1110,  0.0746,  ..., -0.1018, -0.2555,  0.1416],\n",
      "         ...,\n",
      "         [ 0.3296,  0.2437,  0.0612,  ..., -0.0690, -0.0140,  0.1475],\n",
      "         [ 0.3155,  0.1983,  0.0661,  ..., -0.0777, -0.0988,  0.1462],\n",
      "         [ 0.2038,  0.1591, -0.0273,  ..., -0.1910, -0.1493,  0.2256]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after softamx tensor([[[0.2862, 0.2947, 0.2028,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2858, 0.2945, 0.2053,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3535, 0.3645, 0.2820,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2810, 0.2410, 0.2849],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2541, 0.2296, 0.2935],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2712, 0.2830, 0.4458]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[ 0.8015,  1.0656,  0.2919,  ..., -0.6671, -0.8833, -0.3760],\n",
      "         [ 0.7962,  1.0578,  0.2842,  ..., -0.6704, -0.8796, -0.3744],\n",
      "         [ 0.8060,  1.0675,  0.2848,  ..., -0.6708, -0.8661, -0.3813],\n",
      "         ...,\n",
      "         [ 0.9200,  1.2558,  0.5271,  ..., -0.5410, -1.0202, -0.3961],\n",
      "         [ 0.9171,  1.2433,  0.4948,  ..., -0.5632, -0.9892, -0.4025],\n",
      "         [ 0.9087,  1.2217,  0.4539,  ..., -0.5844, -0.9441, -0.4081]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[ 0.8015,  1.0656,  0.2919,  ..., -0.6671, -0.8833, -0.3760],\n",
      "         [ 0.7962,  1.0578,  0.2842,  ..., -0.6704, -0.8796, -0.3744],\n",
      "         [ 0.8060,  1.0675,  0.2848,  ..., -0.6708, -0.8661, -0.3813],\n",
      "         ...,\n",
      "         [ 0.9200,  1.2558,  0.5271,  ..., -0.5410, -1.0202, -0.3961],\n",
      "         [ 0.9171,  1.2433,  0.4948,  ..., -0.5632, -0.9892, -0.4025],\n",
      "         [ 0.9087,  1.2217,  0.4539,  ..., -0.5844, -0.9441, -0.4081]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.6127],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.6162],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.6234],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.4601],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.4935],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.5291]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 0.2634,  0.1411,  0.0715,  ..., -0.0142, -0.1264,  0.2459],\n",
      "         [ 0.2541,  0.1395,  0.0641,  ..., -0.0223, -0.1279,  0.2527],\n",
      "         [ 0.3067,  0.0729,  0.1239,  ...,  0.0404, -0.2626,  0.1919],\n",
      "         ...,\n",
      "         [ 0.3496,  0.2071,  0.1274,  ...,  0.0460, -0.0208,  0.1837],\n",
      "         [ 0.3344,  0.1614,  0.1261,  ...,  0.0422, -0.1071,  0.1820],\n",
      "         [ 0.2119,  0.1247,  0.0313,  ..., -0.0654, -0.1609,  0.2627]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([1, 56, 128])\n",
      "chuuj tensor([[[ 0.2634,  0.1411,  0.0715,  ..., -0.0142, -0.1264,  0.2459],\n",
      "         [ 0.2541,  0.1395,  0.0641,  ..., -0.0223, -0.1279,  0.2527],\n",
      "         [ 0.3067,  0.0729,  0.1239,  ...,  0.0404, -0.2626,  0.1919],\n",
      "         ...,\n",
      "         [ 0.3496,  0.2071,  0.1274,  ...,  0.0460, -0.0208,  0.1837],\n",
      "         [ 0.3344,  0.1614,  0.1261,  ...,  0.0422, -0.1071,  0.1820],\n",
      "         [ 0.2119,  0.1247,  0.0313,  ..., -0.0654, -0.1609,  0.2627]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after softamx tensor([[[0.2604, 0.2573, 0.2267,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2602, 0.2571, 0.2294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3563, 0.3507, 0.2930,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2798, 0.2500, 0.2113],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2855, 0.2539, 0.2081],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.3653, 0.3459, 0.2888]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[-0.5898,  0.3966,  0.6547,  ...,  0.3083,  0.7532, -0.2598],\n",
      "         [-0.5878,  0.3912,  0.6535,  ...,  0.3216,  0.7630, -0.2549],\n",
      "         [-0.6068,  0.4055,  0.6978,  ...,  0.3256,  0.7273, -0.2531],\n",
      "         ...,\n",
      "         [-0.5828,  0.5303,  0.5433,  ..., -0.1822,  0.4933, -0.4271],\n",
      "         [-0.6054,  0.5258,  0.6050,  ..., -0.1039,  0.5014, -0.4024],\n",
      "         [-0.6170,  0.5290,  0.6385,  ..., -0.0812,  0.4865, -0.3932]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[-0.5898,  0.3966,  0.6547,  ...,  0.3083,  0.7532, -0.2598],\n",
      "         [-0.5878,  0.3912,  0.6535,  ...,  0.3216,  0.7630, -0.2549],\n",
      "         [-0.6068,  0.4055,  0.6978,  ...,  0.3256,  0.7273, -0.2531],\n",
      "         ...,\n",
      "         [-0.5828,  0.5303,  0.5433,  ..., -0.1822,  0.4933, -0.4271],\n",
      "         [-0.6054,  0.5258,  0.6050,  ..., -0.1039,  0.5014, -0.4024],\n",
      "         [-0.6170,  0.5290,  0.6385,  ..., -0.0812,  0.4865, -0.3932]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[0.6258, 0.0000, 0.0000,  ..., 0.0000, 0.4871, 0.6335],\n",
      "         [0.6231, 0.0000, 0.0000,  ..., 0.0000, 0.4853, 0.6347],\n",
      "         [0.6066, 0.0000, 0.0000,  ..., 0.0000, 0.5226, 0.6329],\n",
      "         ...,\n",
      "         [0.7545, 0.0000, 0.0000,  ..., 0.0000, 0.4254, 0.5757],\n",
      "         [0.7231, 0.0000, 0.0000,  ..., 0.0000, 0.4714, 0.5859],\n",
      "         [0.7048, 0.0000, 0.0000,  ..., 0.0000, 0.4988, 0.5855]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 0.2423,  0.1631, -0.0025,  ..., -0.0156, -0.1203,  0.1833],\n",
      "         [ 0.2343,  0.1618, -0.0101,  ..., -0.0214, -0.1217,  0.1900],\n",
      "         [ 0.2728,  0.0947,  0.0466,  ...,  0.0053, -0.2551,  0.1367],\n",
      "         ...,\n",
      "         [ 0.3209,  0.2173,  0.0581,  ...,  0.0380, -0.0151,  0.1399],\n",
      "         [ 0.3017,  0.1741,  0.0534,  ...,  0.0217, -0.0982,  0.1425],\n",
      "         [ 0.1892,  0.1412, -0.0456,  ..., -0.0679, -0.1461,  0.2336]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([1, 56, 128])\n",
      "chuuj tensor([[[ 0.2423,  0.1631, -0.0025,  ..., -0.0156, -0.1203,  0.1833],\n",
      "         [ 0.2343,  0.1618, -0.0101,  ..., -0.0214, -0.1217,  0.1900],\n",
      "         [ 0.2728,  0.0947,  0.0466,  ...,  0.0053, -0.2551,  0.1367],\n",
      "         ...,\n",
      "         [ 0.3209,  0.2173,  0.0581,  ...,  0.0380, -0.0151,  0.1399],\n",
      "         [ 0.3017,  0.1741,  0.0534,  ...,  0.0217, -0.0982,  0.1425],\n",
      "         [ 0.1892,  0.1412, -0.0456,  ..., -0.0679, -0.1461,  0.2336]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after softamx tensor([[[0.2779, 0.2855, 0.2167,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2779, 0.2856, 0.2179,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3540, 0.3641, 0.2818,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2609, 0.2371, 0.3034],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2478, 0.2277, 0.3021],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.3028, 0.2881, 0.4091]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[-0.3045, -0.9394, -0.4365,  ..., -0.0361,  0.5807,  0.0474],\n",
      "         [-0.3004, -0.9362, -0.4341,  ..., -0.0277,  0.5711,  0.0469],\n",
      "         [-0.3242, -0.9423, -0.4109,  ..., -0.0429,  0.6140,  0.0316],\n",
      "         ...,\n",
      "         [-0.3749, -1.0104, -0.5646,  ..., -0.2846,  0.7896,  0.1036],\n",
      "         [-0.3885, -1.0098, -0.5257,  ..., -0.2627,  0.8019,  0.0809],\n",
      "         [-0.4044, -1.0064, -0.4880,  ..., -0.2524,  0.8225,  0.0597]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[-0.3045, -0.9394, -0.4365,  ..., -0.0361,  0.5807,  0.0474],\n",
      "         [-0.3004, -0.9362, -0.4341,  ..., -0.0277,  0.5711,  0.0469],\n",
      "         [-0.3242, -0.9423, -0.4109,  ..., -0.0429,  0.6140,  0.0316],\n",
      "         ...,\n",
      "         [-0.3749, -1.0104, -0.5646,  ..., -0.2846,  0.7896,  0.1036],\n",
      "         [-0.3885, -1.0098, -0.5257,  ..., -0.2627,  0.8019,  0.0809],\n",
      "         [-0.4044, -1.0064, -0.4880,  ..., -0.2524,  0.8225,  0.0597]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[0.3204, 0.0000, 0.2858,  ..., 1.6584, 0.7880, 0.8267],\n",
      "         [0.3230, 0.0000, 0.2844,  ..., 1.6595, 0.7868, 0.8233],\n",
      "         [0.2913, 0.0000, 0.2774,  ..., 1.6227, 0.7869, 0.8400],\n",
      "         ...,\n",
      "         [0.2976, 0.0000, 0.3424,  ..., 1.6700, 0.8068, 0.8843],\n",
      "         [0.2728, 0.0000, 0.3289,  ..., 1.6407, 0.8078, 0.8947],\n",
      "         [0.2421, 0.0000, 0.3152,  ..., 1.5982, 0.8038, 0.9040]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 0.3360,  0.1398,  0.0778,  ..., -0.0825, -0.0994,  0.2132],\n",
      "         [ 0.3272,  0.1382,  0.0700,  ..., -0.0918, -0.1005,  0.2196],\n",
      "         [ 0.3754,  0.0746,  0.1289,  ..., -0.0303, -0.2383,  0.1647],\n",
      "         ...,\n",
      "         [ 0.4167,  0.1997,  0.1425,  ..., -0.0014, -0.0037,  0.1692],\n",
      "         [ 0.4036,  0.1564,  0.1377,  ..., -0.0109, -0.0857,  0.1686],\n",
      "         [ 0.2917,  0.1199,  0.0370,  ..., -0.1320, -0.1328,  0.2512]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([1, 56, 128])\n",
      "chuuj tensor([[[ 0.3360,  0.1398,  0.0778,  ..., -0.0825, -0.0994,  0.2132],\n",
      "         [ 0.3272,  0.1382,  0.0700,  ..., -0.0918, -0.1005,  0.2196],\n",
      "         [ 0.3754,  0.0746,  0.1289,  ..., -0.0303, -0.2383,  0.1647],\n",
      "         ...,\n",
      "         [ 0.4167,  0.1997,  0.1425,  ..., -0.0014, -0.0037,  0.1692],\n",
      "         [ 0.4036,  0.1564,  0.1377,  ..., -0.0109, -0.0857,  0.1686],\n",
      "         [ 0.2917,  0.1199,  0.0370,  ..., -0.1320, -0.1328,  0.2512]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after softamx tensor([[[0.2429, 0.2315, 0.2359,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2442, 0.2327, 0.2378,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3450, 0.3279, 0.3270,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2833, 0.2427, 0.1330],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.3179, 0.2690, 0.1396],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.4427, 0.3730, 0.1843]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[-1.6894,  1.5260,  0.6855,  ...,  1.0001, -0.6350, -1.4724],\n",
      "         [-1.6903,  1.5305,  0.6854,  ...,  0.9888, -0.6323, -1.4707],\n",
      "         [-1.7204,  1.5408,  0.7523,  ...,  0.9976, -0.6643, -1.4309],\n",
      "         ...,\n",
      "         [-1.4988,  1.2797,  0.4344,  ...,  1.3404, -0.5994, -1.6144],\n",
      "         [-1.5778,  1.3397,  0.5642,  ...,  1.3098, -0.6518, -1.5527],\n",
      "         [-1.5856,  1.3338,  0.5931,  ...,  1.3258, -0.6682, -1.5315]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[-1.6894,  1.5260,  0.6855,  ...,  1.0001, -0.6350, -1.4724],\n",
      "         [-1.6903,  1.5305,  0.6854,  ...,  0.9888, -0.6323, -1.4707],\n",
      "         [-1.7204,  1.5408,  0.7523,  ...,  0.9976, -0.6643, -1.4309],\n",
      "         ...,\n",
      "         [-1.4988,  1.2797,  0.4344,  ...,  1.3404, -0.5994, -1.6144],\n",
      "         [-1.5778,  1.3397,  0.5642,  ...,  1.3098, -0.6518, -1.5527],\n",
      "         [-1.5856,  1.3338,  0.5931,  ...,  1.3258, -0.6682, -1.5315]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[1.2464, 0.0000, 1.1320,  ..., 0.0000, 0.4403, 0.0000],\n",
      "         [1.2528, 0.0000, 1.1340,  ..., 0.0000, 0.4352, 0.0000],\n",
      "         [1.2537, 0.0000, 1.1438,  ..., 0.0000, 0.4391, 0.0000],\n",
      "         ...,\n",
      "         [0.9711, 0.0000, 0.9889,  ..., 0.1359, 0.5899, 0.0000],\n",
      "         [1.0208, 0.0000, 1.0297,  ..., 0.1192, 0.5773, 0.0000],\n",
      "         [1.0088, 0.0000, 1.0283,  ..., 0.1305, 0.5842, 0.0000]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 0.1456,  0.0570,  0.0320,  ..., -0.0602, -0.0972,  0.2396],\n",
      "         [ 0.1382,  0.0552,  0.0237,  ..., -0.0693, -0.0984,  0.2459],\n",
      "         [ 0.1906, -0.0035,  0.0901,  ...,  0.0015, -0.2315,  0.1890],\n",
      "         ...,\n",
      "         [ 0.2102,  0.1217,  0.0978,  ...,  0.0119,  0.0039,  0.1942],\n",
      "         [ 0.2018,  0.0803,  0.0958,  ...,  0.0095, -0.0789,  0.1938],\n",
      "         [ 0.1032,  0.0428, -0.0100,  ..., -0.1068, -0.1248,  0.2766]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([1, 56, 128])\n",
      "chuuj tensor([[[ 0.1456,  0.0570,  0.0320,  ..., -0.0602, -0.0972,  0.2396],\n",
      "         [ 0.1382,  0.0552,  0.0237,  ..., -0.0693, -0.0984,  0.2459],\n",
      "         [ 0.1906, -0.0035,  0.0901,  ...,  0.0015, -0.2315,  0.1890],\n",
      "         ...,\n",
      "         [ 0.2102,  0.1217,  0.0978,  ...,  0.0119,  0.0039,  0.1942],\n",
      "         [ 0.2018,  0.0803,  0.0958,  ...,  0.0095, -0.0789,  0.1938],\n",
      "         [ 0.1032,  0.0428, -0.0100,  ..., -0.1068, -0.1248,  0.2766]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after softamx tensor([[[0.1971, 0.1848, 0.3012,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1969, 0.1846, 0.3006,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3009, 0.2801, 0.4190,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2280, 0.2560, 0.1277],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2709, 0.2912, 0.1326],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.3981, 0.4185, 0.1834]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[-2.9265,  1.1784,  1.8641,  ...,  0.7864,  1.6030, -0.0547],\n",
      "         [-2.9165,  1.1739,  1.8641,  ...,  0.7790,  1.6079, -0.0357],\n",
      "         [-2.9226,  1.1945,  1.8602,  ...,  0.7701,  1.5848, -0.0545],\n",
      "         ...,\n",
      "         [-3.1367,  1.2110,  1.8174,  ...,  1.0426,  1.4691, -0.6205],\n",
      "         [-3.1487,  1.2512,  1.8350,  ...,  1.0029,  1.4655, -0.5784],\n",
      "         [-3.1567,  1.2640,  1.8278,  ...,  1.0066,  1.4425, -0.6140]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[-2.9265,  1.1784,  1.8641,  ...,  0.7864,  1.6030, -0.0547],\n",
      "         [-2.9165,  1.1739,  1.8641,  ...,  0.7790,  1.6079, -0.0357],\n",
      "         [-2.9226,  1.1945,  1.8602,  ...,  0.7701,  1.5848, -0.0545],\n",
      "         ...,\n",
      "         [-3.1367,  1.2110,  1.8174,  ...,  1.0426,  1.4691, -0.6205],\n",
      "         [-3.1487,  1.2512,  1.8350,  ...,  1.0029,  1.4655, -0.5784],\n",
      "         [-3.1567,  1.2640,  1.8278,  ...,  1.0066,  1.4425, -0.6140]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[0.0000, 0.0000, 0.0000,  ..., 1.7459, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.7455, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.7208, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.7971, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.7670, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.7507, 0.0000, 0.0000]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 0.2521,  0.1318,  0.0496,  ..., -0.1493, -0.0277,  0.2569],\n",
      "         [ 0.2433,  0.1309,  0.0418,  ..., -0.1587, -0.0293,  0.2639],\n",
      "         [ 0.2913,  0.0616,  0.1034,  ..., -0.0931, -0.1568,  0.2064],\n",
      "         ...,\n",
      "         [ 0.3360,  0.1792,  0.1093,  ..., -0.0700,  0.0656,  0.2037],\n",
      "         [ 0.3204,  0.1351,  0.1085,  ..., -0.0792, -0.0118,  0.2050],\n",
      "         [ 0.2035,  0.1028,  0.0108,  ..., -0.2026, -0.0640,  0.2954]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([1, 56, 128])\n",
      "chuuj tensor([[[ 0.2521,  0.1318,  0.0496,  ..., -0.1493, -0.0277,  0.2569],\n",
      "         [ 0.2433,  0.1309,  0.0418,  ..., -0.1587, -0.0293,  0.2639],\n",
      "         [ 0.2913,  0.0616,  0.1034,  ..., -0.0931, -0.1568,  0.2064],\n",
      "         ...,\n",
      "         [ 0.3360,  0.1792,  0.1093,  ..., -0.0700,  0.0656,  0.2037],\n",
      "         [ 0.3204,  0.1351,  0.1085,  ..., -0.0792, -0.0118,  0.2050],\n",
      "         [ 0.2035,  0.1028,  0.0108,  ..., -0.2026, -0.0640,  0.2954]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "obs = rllib_env.reset()\n",
    "substation_prob_distr, node_embeddings, substation_embeddings = enc(vectorize_obs(obs, env).unsqueeze(0), torch.from_numpy(obs[\"connectivity_matrix\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_node_embeddings_idx = sub_id_to_elem_id[sampled_sub]\n",
    "subset_node_embeddings = node_embeddings[:, subset_node_embeddings_idx, :]\n",
    "\n",
    "substation_embeddings.shape, subset_node_embeddings.shape\n",
    "\n",
    "context_embedding = torch.mean(substation_embeddings, dim = 1, keepdim=True).expand(subset_node_embeddings.shape)\n",
    "enriched_node_embedding = torch.cat((context_embedding, subset_node_embeddings), dim = 2)\n",
    "\n",
    "enriched_node_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after softamx tensor([[[0.2292, 0.3924, 0.3785, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2418, 0.3863, 0.3719, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2414, 0.3864, 0.3722, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1488, 0.1379, 0.1640, 0.1934, 0.1825, 0.1734],\n",
      "         [0.1485, 0.1373, 0.1639, 0.1942, 0.1830, 0.1731],\n",
      "         [0.1470, 0.1368, 0.1642, 0.1938, 0.1825, 0.1757],\n",
      "         [0.1465, 0.1372, 0.1645, 0.1927, 0.1818, 0.1773],\n",
      "         [0.1468, 0.1372, 0.1644, 0.1930, 0.1820, 0.1766],\n",
      "         [0.1458, 0.1359, 0.1643, 0.1943, 0.1827, 0.1770]],\n",
      "\n",
      "        [[0.2430, 0.2142, 0.2759, 0.2669, 0.0000, 0.0000],\n",
      "         [0.2431, 0.2123, 0.2775, 0.2671, 0.0000, 0.0000],\n",
      "         [0.2418, 0.2154, 0.2747, 0.2681, 0.0000, 0.0000],\n",
      "         [0.2414, 0.2142, 0.2755, 0.2689, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1384, 0.1571, 0.2323, 0.1774, 0.1600, 0.1349],\n",
      "         [0.1356, 0.1570, 0.2347, 0.1766, 0.1612, 0.1348],\n",
      "         [0.1333, 0.1579, 0.2327, 0.1743, 0.1645, 0.1374],\n",
      "         [0.1357, 0.1574, 0.2328, 0.1759, 0.1622, 0.1361],\n",
      "         [0.1348, 0.1569, 0.2359, 0.1765, 0.1614, 0.1345],\n",
      "         [0.1364, 0.1566, 0.2359, 0.1775, 0.1600, 0.1336]],\n",
      "\n",
      "        [[0.1657, 0.2934, 0.1795, 0.1741, 0.1872, 0.0000],\n",
      "         [0.1566, 0.2946, 0.1848, 0.1788, 0.1852, 0.0000],\n",
      "         [0.1606, 0.3001, 0.1797, 0.1738, 0.1858, 0.0000],\n",
      "         [0.1610, 0.3001, 0.1794, 0.1736, 0.1859, 0.0000],\n",
      "         [0.1617, 0.2969, 0.1804, 0.1747, 0.1862, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0784, 0.0625, 0.1727, 0.1950, 0.3654, 0.1260],\n",
      "         [0.0761, 0.0588, 0.1725, 0.1947, 0.3737, 0.1242],\n",
      "         [0.0626, 0.0555, 0.1688, 0.1988, 0.3965, 0.1177],\n",
      "         [0.0608, 0.0547, 0.1683, 0.1993, 0.4001, 0.1168],\n",
      "         [0.0738, 0.0612, 0.1862, 0.2154, 0.3321, 0.1312],\n",
      "         [0.0663, 0.0564, 0.1699, 0.1977, 0.3901, 0.1196]],\n",
      "\n",
      "        [[0.4091, 0.2759, 0.3150, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4266, 0.2634, 0.3100, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4212, 0.2672, 0.3116, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2720, 0.7280, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2639, 0.7361, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1297, 0.1777, 0.3239, 0.2223, 0.1465, 0.0000],\n",
      "         [0.1247, 0.1725, 0.3334, 0.2234, 0.1460, 0.0000],\n",
      "         [0.1139, 0.1621, 0.3550, 0.2256, 0.1435, 0.0000],\n",
      "         [0.1187, 0.1674, 0.3453, 0.2246, 0.1439, 0.0000],\n",
      "         [0.1236, 0.1732, 0.3356, 0.2237, 0.1439, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2437, 0.4760, 0.2803, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2347, 0.4688, 0.2965, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2348, 0.4831, 0.2821, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1934, 0.4919, 0.3147, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1761, 0.4847, 0.3392, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1755, 0.4970, 0.3275, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1954, 0.2066, 0.5980, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1823, 0.1997, 0.6180, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1787, 0.2397, 0.5816, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1148, 0.2089, 0.4964, 0.1799, 0.0000, 0.0000],\n",
      "         [0.0969, 0.1932, 0.5600, 0.1499, 0.0000, 0.0000],\n",
      "         [0.1112, 0.2244, 0.4844, 0.1800, 0.0000, 0.0000],\n",
      "         [0.1091, 0.2017, 0.5240, 0.1652, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1790, 0.4968, 0.3242, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1646, 0.5039, 0.3315, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1660, 0.5060, 0.3280, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[-0.8479,  0.4720,  2.4424,  ...,  3.5921,  1.3599,  0.6131],\n",
      "         [-0.8455,  0.4695,  2.4408,  ...,  3.5884,  1.3606,  0.6117],\n",
      "         [-0.8456,  0.4696,  2.4409,  ...,  3.5885,  1.3605,  0.6118],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.8522,  0.4857,  2.4150,  ...,  3.5896,  1.3787,  0.5908],\n",
      "         [-0.8523,  0.4857,  2.4152,  ...,  3.5898,  1.3787,  0.5909],\n",
      "         [-0.8523,  0.4854,  2.4158,  ...,  3.5900,  1.3785,  0.5913],\n",
      "         [-0.8522,  0.4851,  2.4160,  ...,  3.5899,  1.3785,  0.5915],\n",
      "         [-0.8522,  0.4852,  2.4159,  ...,  3.5899,  1.3785,  0.5914],\n",
      "         [-0.8524,  0.4852,  2.4163,  ...,  3.5903,  1.3783,  0.5917]],\n",
      "\n",
      "        [[-0.8124,  0.4625,  2.3538,  ...,  3.5159,  1.4024,  0.5358],\n",
      "         [-0.8126,  0.4627,  2.3539,  ...,  3.5161,  1.4024,  0.5358],\n",
      "         [-0.8123,  0.4622,  2.3540,  ...,  3.5158,  1.4024,  0.5359],\n",
      "         [-0.8124,  0.4623,  2.3541,  ...,  3.5160,  1.4023,  0.5360],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.9556,  0.5118,  2.6915,  ...,  3.8105,  1.2105,  0.8492],\n",
      "         [-0.9617,  0.5148,  2.7019,  ...,  3.8214,  1.2057,  0.8581],\n",
      "         [-0.9485,  0.5012,  2.6927,  ...,  3.8015,  1.2111,  0.8490],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.3744,  0.4295,  3.1186,  ...,  4.0140,  1.3855,  1.2063],\n",
      "         [-1.4352,  0.4174,  3.2059,  ...,  4.0518,  1.3949,  1.2767],\n",
      "         [-1.3641,  0.4302,  3.1077,  ...,  4.0100,  1.3820,  1.1970],\n",
      "         [-1.4005,  0.4240,  3.1560,  ...,  4.0299,  1.3898,  1.2364],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.9905,  0.5292,  2.7303,  ...,  3.8759,  1.2251,  0.8731],\n",
      "         [-0.9933,  0.5287,  2.7385,  ...,  3.8820,  1.2219,  0.8795],\n",
      "         [-0.9933,  0.5289,  2.7382,  ...,  3.8819,  1.2220,  0.8793],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[-0.8479,  0.4720,  2.4424,  ...,  3.5921,  1.3599,  0.6131],\n",
      "         [-0.8455,  0.4695,  2.4408,  ...,  3.5884,  1.3606,  0.6117],\n",
      "         [-0.8456,  0.4696,  2.4409,  ...,  3.5885,  1.3605,  0.6118],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.8522,  0.4857,  2.4150,  ...,  3.5896,  1.3787,  0.5908],\n",
      "         [-0.8523,  0.4857,  2.4152,  ...,  3.5898,  1.3787,  0.5909],\n",
      "         [-0.8523,  0.4854,  2.4158,  ...,  3.5900,  1.3785,  0.5913],\n",
      "         [-0.8522,  0.4851,  2.4160,  ...,  3.5899,  1.3785,  0.5915],\n",
      "         [-0.8522,  0.4852,  2.4159,  ...,  3.5899,  1.3785,  0.5914],\n",
      "         [-0.8524,  0.4852,  2.4163,  ...,  3.5903,  1.3783,  0.5917]],\n",
      "\n",
      "        [[-0.8124,  0.4625,  2.3538,  ...,  3.5159,  1.4024,  0.5358],\n",
      "         [-0.8126,  0.4627,  2.3539,  ...,  3.5161,  1.4024,  0.5358],\n",
      "         [-0.8123,  0.4622,  2.3540,  ...,  3.5158,  1.4024,  0.5359],\n",
      "         [-0.8124,  0.4623,  2.3541,  ...,  3.5160,  1.4023,  0.5360],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.9556,  0.5118,  2.6915,  ...,  3.8105,  1.2105,  0.8492],\n",
      "         [-0.9617,  0.5148,  2.7019,  ...,  3.8214,  1.2057,  0.8581],\n",
      "         [-0.9485,  0.5012,  2.6927,  ...,  3.8015,  1.2111,  0.8490],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.3744,  0.4295,  3.1186,  ...,  4.0140,  1.3855,  1.2063],\n",
      "         [-1.4352,  0.4174,  3.2059,  ...,  4.0518,  1.3949,  1.2767],\n",
      "         [-1.3641,  0.4302,  3.1077,  ...,  4.0100,  1.3820,  1.1970],\n",
      "         [-1.4005,  0.4240,  3.1560,  ...,  4.0299,  1.3898,  1.2364],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.9905,  0.5292,  2.7303,  ...,  3.8759,  1.2251,  0.8731],\n",
      "         [-0.9933,  0.5287,  2.7385,  ...,  3.8820,  1.2219,  0.8795],\n",
      "         [-0.9933,  0.5289,  2.7382,  ...,  3.8819,  1.2220,  0.8793],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[0.0000, 0.0000, 0.0000,  ..., 1.4618, 0.0000, 0.4551],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.4621, 0.0000, 0.4566],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.4621, 0.0000, 0.4566],\n",
      "         [0.0543, 0.1184, 0.0000,  ..., 0.0000, 0.1109, 0.0000],\n",
      "         [0.0543, 0.1184, 0.0000,  ..., 0.0000, 0.1109, 0.0000],\n",
      "         [0.0543, 0.1184, 0.0000,  ..., 0.0000, 0.1109, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 1.4775, 0.0000, 0.4562],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.4775, 0.0000, 0.4561],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.4774, 0.0000, 0.4560],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.4774, 0.0000, 0.4560],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.4774, 0.0000, 0.4560],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.4773, 0.0000, 0.4558]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 1.4918, 0.0000, 0.4885],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.4918, 0.0000, 0.4884],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.4918, 0.0000, 0.4885],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.4917, 0.0000, 0.4884],\n",
      "         [0.0543, 0.1184, 0.0000,  ..., 0.0000, 0.1109, 0.0000],\n",
      "         [0.0543, 0.1184, 0.0000,  ..., 0.0000, 0.1109, 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 1.3500, 0.0000, 0.3257],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.3467, 0.0000, 0.3197],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.3501, 0.0000, 0.3292],\n",
      "         [0.0543, 0.1184, 0.0000,  ..., 0.0000, 0.1109, 0.0000],\n",
      "         [0.0543, 0.1184, 0.0000,  ..., 0.0000, 0.1109, 0.0000],\n",
      "         [0.0543, 0.1184, 0.0000,  ..., 0.0000, 0.1109, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 1.3016, 0.0000, 0.2687],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.2828, 0.0000, 0.2489],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.3039, 0.0000, 0.2707],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.2936, 0.0000, 0.2604],\n",
      "         [0.0543, 0.1184, 0.0000,  ..., 0.0000, 0.1109, 0.0000],\n",
      "         [0.0543, 0.1184, 0.0000,  ..., 0.0000, 0.1109, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 1.3655, 0.0000, 0.3037],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.3632, 0.0000, 0.3002],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.3632, 0.0000, 0.3002],\n",
      "         [0.0543, 0.1184, 0.0000,  ..., 0.0000, 0.1109, 0.0000],\n",
      "         [0.0543, 0.1184, 0.0000,  ..., 0.0000, 0.1109, 0.0000],\n",
      "         [0.0543, 0.1184, 0.0000,  ..., 0.0000, 0.1109, 0.0000]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 0.2425,  0.0818,  0.0524,  ..., -0.1148, -0.1971,  0.2377],\n",
      "         [ 0.2415,  0.0825,  0.0523,  ..., -0.1638, -0.1043,  0.2803],\n",
      "         [ 0.2416,  0.0826,  0.0524,  ..., -0.1710, -0.1059,  0.2856],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055]],\n",
      "\n",
      "        [[ 0.2427,  0.0832,  0.0523,  ..., -0.2068, -0.1266,  0.3085],\n",
      "         [ 0.2428,  0.0829,  0.0522,  ..., -0.1787, -0.1483,  0.2863],\n",
      "         [ 0.2423,  0.0828,  0.0520,  ..., -0.1579, -0.1317,  0.2722],\n",
      "         [ 0.2419,  0.0831,  0.0520,  ..., -0.1734, -0.1016,  0.2867],\n",
      "         [ 0.2421,  0.0831,  0.0520,  ..., -0.1744, -0.1093,  0.2867],\n",
      "         [ 0.2421,  0.0825,  0.0516,  ..., -0.1133, -0.1446,  0.2382]],\n",
      "\n",
      "        [[ 0.2438,  0.0828,  0.0517,  ..., -0.1960, -0.1365,  0.2996],\n",
      "         [ 0.2439,  0.0824,  0.0515,  ..., -0.1475, -0.1745,  0.2613],\n",
      "         [ 0.2434,  0.0829,  0.0516,  ..., -0.1877, -0.1206,  0.2954],\n",
      "         [ 0.2434,  0.0825,  0.0513,  ..., -0.1432, -0.1452,  0.2599],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2370,  0.0836,  0.0559,  ..., -0.1962, -0.1256,  0.3073],\n",
      "         [ 0.2363,  0.0823,  0.0552,  ..., -0.0410, -0.1907,  0.1883],\n",
      "         [ 0.2339,  0.0834,  0.0546,  ..., -0.1159, -0.0083,  0.2571],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055]],\n",
      "\n",
      "        [[ 0.2284,  0.0752,  0.0599,  ..., -0.1877, -0.1364,  0.3034],\n",
      "         [ 0.2251,  0.0741,  0.0604,  ..., -0.0954, -0.0889,  0.2356],\n",
      "         [ 0.2302,  0.0709,  0.0623,  ...,  0.2910, -0.1835,  0.2124],\n",
      "         [ 0.2269,  0.0754,  0.0601,  ..., -0.2152, -0.0535,  0.3278],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055]],\n",
      "\n",
      "        [[ 0.2372,  0.0836,  0.0558,  ..., -0.1998, -0.1259,  0.3064],\n",
      "         [ 0.2340,  0.0836,  0.0545,  ..., -0.1021, -0.0267,  0.2436],\n",
      "         [ 0.2353,  0.0833,  0.0549,  ..., -0.1060, -0.0823,  0.2410],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([14, 6, 256])\n",
      "chuuj tensor([[[ 0.2425,  0.0818,  0.0524,  ..., -0.1148, -0.1971,  0.2377],\n",
      "         [ 0.2415,  0.0825,  0.0523,  ..., -0.1638, -0.1043,  0.2803],\n",
      "         [ 0.2416,  0.0826,  0.0524,  ..., -0.1710, -0.1059,  0.2856],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055]],\n",
      "\n",
      "        [[ 0.2427,  0.0832,  0.0523,  ..., -0.2068, -0.1266,  0.3085],\n",
      "         [ 0.2428,  0.0829,  0.0522,  ..., -0.1787, -0.1483,  0.2863],\n",
      "         [ 0.2423,  0.0828,  0.0520,  ..., -0.1579, -0.1317,  0.2722],\n",
      "         [ 0.2419,  0.0831,  0.0520,  ..., -0.1734, -0.1016,  0.2867],\n",
      "         [ 0.2421,  0.0831,  0.0520,  ..., -0.1744, -0.1093,  0.2867],\n",
      "         [ 0.2421,  0.0825,  0.0516,  ..., -0.1133, -0.1446,  0.2382]],\n",
      "\n",
      "        [[ 0.2438,  0.0828,  0.0517,  ..., -0.1960, -0.1365,  0.2996],\n",
      "         [ 0.2439,  0.0824,  0.0515,  ..., -0.1475, -0.1745,  0.2613],\n",
      "         [ 0.2434,  0.0829,  0.0516,  ..., -0.1877, -0.1206,  0.2954],\n",
      "         [ 0.2434,  0.0825,  0.0513,  ..., -0.1432, -0.1452,  0.2599],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2370,  0.0836,  0.0559,  ..., -0.1962, -0.1256,  0.3073],\n",
      "         [ 0.2363,  0.0823,  0.0552,  ..., -0.0410, -0.1907,  0.1883],\n",
      "         [ 0.2339,  0.0834,  0.0546,  ..., -0.1159, -0.0083,  0.2571],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055]],\n",
      "\n",
      "        [[ 0.2284,  0.0752,  0.0599,  ..., -0.1877, -0.1364,  0.3034],\n",
      "         [ 0.2251,  0.0741,  0.0604,  ..., -0.0954, -0.0889,  0.2356],\n",
      "         [ 0.2302,  0.0709,  0.0623,  ...,  0.2910, -0.1835,  0.2124],\n",
      "         [ 0.2269,  0.0754,  0.0601,  ..., -0.2152, -0.0535,  0.3278],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055]],\n",
      "\n",
      "        [[ 0.2372,  0.0836,  0.0558,  ..., -0.1998, -0.1259,  0.3064],\n",
      "         [ 0.2340,  0.0836,  0.0545,  ..., -0.1021, -0.0267,  0.2436],\n",
      "         [ 0.2353,  0.0833,  0.0549,  ..., -0.1060, -0.0823,  0.2410],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055],\n",
      "         [-0.0017,  0.0010, -0.0015,  ..., -0.0058,  0.0015,  0.0055]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after softamx tensor([[[0.3705, 0.3110, 0.3184, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3725, 0.3095, 0.3181, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3724, 0.3095, 0.3181, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1852, 0.1852, 0.1647, 0.1548, 0.1596, 0.1504],\n",
      "         [0.1843, 0.1847, 0.1649, 0.1551, 0.1597, 0.1513],\n",
      "         [0.1844, 0.1849, 0.1649, 0.1549, 0.1596, 0.1513],\n",
      "         [0.1854, 0.1855, 0.1647, 0.1546, 0.1594, 0.1503],\n",
      "         [0.1852, 0.1854, 0.1648, 0.1547, 0.1595, 0.1505],\n",
      "         [0.1833, 0.1843, 0.1652, 0.1551, 0.1597, 0.1524]],\n",
      "\n",
      "        [[0.2619, 0.2619, 0.2432, 0.2329, 0.0000, 0.0000],\n",
      "         [0.2605, 0.2620, 0.2430, 0.2344, 0.0000, 0.0000],\n",
      "         [0.2622, 0.2620, 0.2432, 0.2327, 0.0000, 0.0000],\n",
      "         [0.2610, 0.2621, 0.2430, 0.2339, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1935, 0.1674, 0.1386, 0.1642, 0.1592, 0.1772],\n",
      "         [0.1920, 0.1676, 0.1391, 0.1639, 0.1600, 0.1774],\n",
      "         [0.1938, 0.1675, 0.1379, 0.1640, 0.1592, 0.1776],\n",
      "         [0.1936, 0.1675, 0.1382, 0.1641, 0.1592, 0.1774],\n",
      "         [0.1909, 0.1678, 0.1396, 0.1638, 0.1605, 0.1775],\n",
      "         [0.1907, 0.1677, 0.1399, 0.1639, 0.1605, 0.1773]],\n",
      "\n",
      "        [[0.2372, 0.1603, 0.1966, 0.2005, 0.2055, 0.0000],\n",
      "         [0.2371, 0.1593, 0.1969, 0.2010, 0.2056, 0.0000],\n",
      "         [0.2322, 0.1620, 0.1984, 0.2021, 0.2053, 0.0000],\n",
      "         [0.2322, 0.1621, 0.1984, 0.2021, 0.2053, 0.0000],\n",
      "         [0.2352, 0.1607, 0.1974, 0.2013, 0.2054, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2328, 0.2262, 0.1259, 0.1139, 0.1486, 0.1525],\n",
      "         [0.2249, 0.2228, 0.1313, 0.1200, 0.1447, 0.1563],\n",
      "         [0.2295, 0.2273, 0.1283, 0.1163, 0.1435, 0.1551],\n",
      "         [0.2281, 0.2270, 0.1293, 0.1173, 0.1423, 0.1559],\n",
      "         [0.2339, 0.1897, 0.1276, 0.1141, 0.1865, 0.1482],\n",
      "         [0.2291, 0.2266, 0.1284, 0.1165, 0.1443, 0.1550]],\n",
      "\n",
      "        [[0.3099, 0.3549, 0.3352, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3069, 0.3567, 0.3364, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3076, 0.3563, 0.3361, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6154, 0.3846, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6245, 0.3755, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2612, 0.2150, 0.1341, 0.1719, 0.2178, 0.0000],\n",
      "         [0.2625, 0.2152, 0.1328, 0.1713, 0.2183, 0.0000],\n",
      "         [0.2579, 0.2137, 0.1356, 0.1735, 0.2193, 0.0000],\n",
      "         [0.2589, 0.2141, 0.1351, 0.1730, 0.2188, 0.0000],\n",
      "         [0.2569, 0.2137, 0.1371, 0.1739, 0.2184, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.4034, 0.2718, 0.3248, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4073, 0.2687, 0.3240, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3967, 0.2738, 0.3295, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.4538, 0.2567, 0.2896, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4590, 0.2527, 0.2883, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4441, 0.2591, 0.2968, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.4345, 0.3383, 0.2272, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4175, 0.3475, 0.2349, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4416, 0.3379, 0.2205, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3215, 0.2086, 0.1961, 0.2738, 0.0000, 0.0000],\n",
      "         [0.3221, 0.2113, 0.1946, 0.2720, 0.0000, 0.0000],\n",
      "         [0.2949, 0.2057, 0.2080, 0.2914, 0.0000, 0.0000],\n",
      "         [0.3242, 0.2043, 0.1975, 0.2741, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.4577, 0.2440, 0.2982, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4635, 0.2394, 0.2971, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4577, 0.2429, 0.2994, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "after matmul tensor([[[ 0.3111,  0.6352, -1.1333,  ...,  0.8080, -1.3089,  0.4428],\n",
      "         [ 0.3104,  0.6352, -1.1334,  ...,  0.8072, -1.3090,  0.4431],\n",
      "         [ 0.3104,  0.6352, -1.1334,  ...,  0.8072, -1.3090,  0.4431],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.3897,  0.6918, -1.1365,  ...,  0.9121, -1.3166,  0.4438],\n",
      "         [ 0.3895,  0.6915, -1.1364,  ...,  0.9118, -1.3165,  0.4438],\n",
      "         [ 0.3895,  0.6915, -1.1364,  ...,  0.9118, -1.3165,  0.4438],\n",
      "         [ 0.3897,  0.6919, -1.1365,  ...,  0.9121, -1.3166,  0.4439],\n",
      "         [ 0.3896,  0.6918, -1.1365,  ...,  0.9121, -1.3166,  0.4439],\n",
      "         [ 0.3893,  0.6911, -1.1364,  ...,  0.9113, -1.3164,  0.4437]],\n",
      "\n",
      "        [[ 0.3462,  0.7216, -1.1484,  ...,  0.8742, -1.3315,  0.4746],\n",
      "         [ 0.3460,  0.7213, -1.1484,  ...,  0.8738, -1.3314,  0.4746],\n",
      "         [ 0.3462,  0.7216, -1.1485,  ...,  0.8742, -1.3315,  0.4746],\n",
      "         [ 0.3460,  0.7214, -1.1484,  ...,  0.8739, -1.3314,  0.4746],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2947,  0.5297, -1.1131,  ...,  0.7675, -1.2720,  0.3849],\n",
      "         [ 0.2921,  0.5208, -1.1113,  ...,  0.7602, -1.2695,  0.3826],\n",
      "         [ 0.2936,  0.5336, -1.1146,  ...,  0.7683, -1.2735,  0.3871],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.2708,  0.4645, -1.1610,  ...,  0.7441, -1.2152,  0.4937],\n",
      "         [ 0.2719,  0.4660, -1.1605,  ...,  0.7451, -1.2158,  0.4930],\n",
      "         [ 0.2630,  0.4444, -1.1623,  ...,  0.7323, -1.2076,  0.4951],\n",
      "         [ 0.2698,  0.4644, -1.1618,  ...,  0.7435, -1.2150,  0.4948],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.4363,  0.5459, -1.1074,  ...,  0.9181, -1.2564,  0.3582],\n",
      "         [ 0.4358,  0.5491, -1.1086,  ...,  0.9192, -1.2575,  0.3598],\n",
      "         [ 0.4361,  0.5460, -1.1075,  ...,  0.9180, -1.2564,  0.3583],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output here tensor([[[ 0.3111,  0.6352, -1.1333,  ...,  0.8080, -1.3089,  0.4428],\n",
      "         [ 0.3104,  0.6352, -1.1334,  ...,  0.8072, -1.3090,  0.4431],\n",
      "         [ 0.3104,  0.6352, -1.1334,  ...,  0.8072, -1.3090,  0.4431],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.3897,  0.6918, -1.1365,  ...,  0.9121, -1.3166,  0.4438],\n",
      "         [ 0.3895,  0.6915, -1.1364,  ...,  0.9118, -1.3165,  0.4438],\n",
      "         [ 0.3895,  0.6915, -1.1364,  ...,  0.9118, -1.3165,  0.4438],\n",
      "         [ 0.3897,  0.6919, -1.1365,  ...,  0.9121, -1.3166,  0.4439],\n",
      "         [ 0.3896,  0.6918, -1.1365,  ...,  0.9121, -1.3166,  0.4439],\n",
      "         [ 0.3893,  0.6911, -1.1364,  ...,  0.9113, -1.3164,  0.4437]],\n",
      "\n",
      "        [[ 0.3462,  0.7216, -1.1484,  ...,  0.8742, -1.3315,  0.4746],\n",
      "         [ 0.3460,  0.7213, -1.1484,  ...,  0.8738, -1.3314,  0.4746],\n",
      "         [ 0.3462,  0.7216, -1.1485,  ...,  0.8742, -1.3315,  0.4746],\n",
      "         [ 0.3460,  0.7214, -1.1484,  ...,  0.8739, -1.3314,  0.4746],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2947,  0.5297, -1.1131,  ...,  0.7675, -1.2720,  0.3849],\n",
      "         [ 0.2921,  0.5208, -1.1113,  ...,  0.7602, -1.2695,  0.3826],\n",
      "         [ 0.2936,  0.5336, -1.1146,  ...,  0.7683, -1.2735,  0.3871],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.2708,  0.4645, -1.1610,  ...,  0.7441, -1.2152,  0.4937],\n",
      "         [ 0.2719,  0.4660, -1.1605,  ...,  0.7451, -1.2158,  0.4930],\n",
      "         [ 0.2630,  0.4444, -1.1623,  ...,  0.7323, -1.2076,  0.4951],\n",
      "         [ 0.2698,  0.4644, -1.1618,  ...,  0.7435, -1.2150,  0.4948],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.4363,  0.5459, -1.1074,  ...,  0.9181, -1.2564,  0.3582],\n",
      "         [ 0.4358,  0.5491, -1.1086,  ...,  0.9192, -1.2575,  0.3598],\n",
      "         [ 0.4361,  0.5460, -1.1075,  ...,  0.9180, -1.2564,  0.3583],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "post relu tensor([[[0.0000, 0.4378, 0.0000,  ..., 0.4150, 1.9963, 0.0000],\n",
      "         [0.0000, 0.4379, 0.0000,  ..., 0.4156, 1.9967, 0.0000],\n",
      "         [0.0000, 0.4379, 0.0000,  ..., 0.4156, 1.9967, 0.0000],\n",
      "         [0.1085, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.1171],\n",
      "         [0.1085, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.1171],\n",
      "         [0.1085, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.1171]],\n",
      "\n",
      "        [[0.0000, 0.4186, 0.0000,  ..., 0.3487, 1.9897, 0.0000],\n",
      "         [0.0000, 0.4186, 0.0000,  ..., 0.3489, 1.9896, 0.0000],\n",
      "         [0.0000, 0.4186, 0.0000,  ..., 0.3489, 1.9896, 0.0000],\n",
      "         [0.0000, 0.4186, 0.0000,  ..., 0.3487, 1.9897, 0.0000],\n",
      "         [0.0000, 0.4186, 0.0000,  ..., 0.3488, 1.9897, 0.0000],\n",
      "         [0.0000, 0.4187, 0.0000,  ..., 0.3492, 1.9896, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.4232, 0.0000,  ..., 0.3888, 2.0241, 0.0000],\n",
      "         [0.0000, 0.4233, 0.0000,  ..., 0.3890, 2.0241, 0.0000],\n",
      "         [0.0000, 0.4232, 0.0000,  ..., 0.3888, 2.0241, 0.0000],\n",
      "         [0.0000, 0.4233, 0.0000,  ..., 0.3890, 2.0241, 0.0000],\n",
      "         [0.1085, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.1171],\n",
      "         [0.1085, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.1171]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000, 0.4517, 0.0000,  ..., 0.4006, 1.9249, 0.0000],\n",
      "         [0.0000, 0.4523, 0.0000,  ..., 0.4040, 1.9228, 0.0000],\n",
      "         [0.0000, 0.4517, 0.0000,  ..., 0.4012, 1.9270, 0.0000],\n",
      "         [0.1085, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.1171],\n",
      "         [0.1085, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.1171],\n",
      "         [0.1085, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.1171]],\n",
      "\n",
      "        [[0.0000, 0.4227, 0.0000,  ..., 0.4034, 1.8002, 0.0000],\n",
      "         [0.0000, 0.4230, 0.0000,  ..., 0.4028, 1.8009, 0.0000],\n",
      "         [0.0000, 0.4210, 0.0000,  ..., 0.4097, 1.7916, 0.0000],\n",
      "         [0.0000, 0.4226, 0.0000,  ..., 0.4040, 1.8000, 0.0000],\n",
      "         [0.1085, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.1171],\n",
      "         [0.1085, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.1171]],\n",
      "\n",
      "        [[0.0000, 0.4220, 0.0000,  ..., 0.2857, 1.8854, 0.0000],\n",
      "         [0.0000, 0.4220, 0.0000,  ..., 0.2858, 1.8869, 0.0000],\n",
      "         [0.0000, 0.4220, 0.0000,  ..., 0.2858, 1.8856, 0.0000],\n",
      "         [0.1085, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.1171],\n",
      "         [0.1085, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.1171],\n",
      "         [0.1085, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.1171]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "post gate tensor([[[ 1.8429e-01,  9.7753e-02,  5.8055e-02,  ..., -8.2816e-02,\n",
      "          -1.3097e-01,  2.4475e-01],\n",
      "         [ 1.8536e-01,  9.7138e-02,  6.0830e-02,  ..., -1.2972e-01,\n",
      "          -3.1096e-02,  2.8740e-01],\n",
      "         [ 1.8516e-01,  9.7068e-02,  6.0915e-02,  ..., -1.3681e-01,\n",
      "          -3.2419e-02,  2.9291e-01],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02]],\n",
      "\n",
      "        [[ 1.8369e-01,  9.5775e-02,  6.2278e-02,  ..., -1.7294e-01,\n",
      "          -5.3587e-02,  3.1780e-01],\n",
      "         [ 1.8377e-01,  9.6069e-02,  6.1317e-02,  ..., -1.4559e-01,\n",
      "          -7.7798e-02,  2.9497e-01],\n",
      "         [ 1.8458e-01,  9.6249e-02,  6.1279e-02,  ..., -1.2496e-01,\n",
      "          -6.1523e-02,  2.8015e-01],\n",
      "         [ 1.8491e-01,  9.6041e-02,  6.2195e-02,  ..., -1.3985e-01,\n",
      "          -2.8913e-02,  2.9470e-01],\n",
      "         [ 1.8472e-01,  9.6045e-02,  6.2045e-02,  ..., -1.4090e-01,\n",
      "          -3.7052e-02,  2.9482e-01],\n",
      "         [ 1.8526e-01,  9.6720e-02,  6.0240e-02,  ..., -8.1285e-02,\n",
      "          -7.7276e-02,  2.4524e-01]],\n",
      "\n",
      "        [[ 1.8387e-01,  9.6251e-02,  6.0784e-02,  ..., -1.6243e-01,\n",
      "          -6.1463e-02,  3.0745e-01],\n",
      "         [ 1.8401e-01,  9.6764e-02,  5.9138e-02,  ..., -1.1523e-01,\n",
      "          -1.0371e-01,  2.6830e-01],\n",
      "         [ 1.8435e-01,  9.6284e-02,  6.0940e-02,  ..., -1.5404e-01,\n",
      "          -4.5166e-02,  3.0291e-01],\n",
      "         [ 1.8477e-01,  9.6773e-02,  5.9671e-02,  ..., -1.1060e-01,\n",
      "          -7.3280e-02,  2.6636e-01],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.8400e-01,  9.6956e-02,  6.1651e-02,  ..., -1.6253e-01,\n",
      "          -6.0149e-02,  3.1747e-01],\n",
      "         [ 1.8600e-01,  9.8714e-02,  5.7533e-02,  ..., -1.1798e-02,\n",
      "          -1.3603e-01,  1.9572e-01],\n",
      "         [ 1.8864e-01,  9.7546e-02,  6.2905e-02,  ..., -8.2362e-02,\n",
      "           6.0695e-02,  2.6332e-01],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02]],\n",
      "\n",
      "        [[ 1.8724e-01,  9.6485e-02,  6.0124e-02,  ..., -1.4839e-01,\n",
      "          -8.7049e-02,  3.0951e-01],\n",
      "         [ 1.9046e-01,  9.7480e-02,  5.9356e-02,  ..., -5.5076e-02,\n",
      "          -4.4060e-02,  2.3835e-01],\n",
      "         [ 1.9188e-01,  9.9505e-02,  5.9974e-02,  ...,  3.1485e-01,\n",
      "          -1.5317e-01,  2.0905e-01],\n",
      "         [ 1.8846e-01,  9.6190e-02,  6.2156e-02,  ..., -1.7415e-01,\n",
      "           2.2214e-04,  3.3336e-01],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02]],\n",
      "\n",
      "        [[ 1.8363e-01,  9.5633e-02,  6.4191e-02,  ..., -1.6576e-01,\n",
      "          -6.3540e-02,  3.1756e-01],\n",
      "         [ 1.8821e-01,  9.6435e-02,  6.4682e-02,  ..., -6.9201e-02,\n",
      "           3.6483e-02,  2.5128e-01],\n",
      "         [ 1.8679e-01,  9.6517e-02,  6.3446e-02,  ..., -7.3595e-02,\n",
      "          -2.2515e-02,  2.4945e-01],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02]]], grad_fn=<AddBackward0>)\n",
      "x shape:  torch.Size([14, 6, 256])\n",
      "chuuj tensor([[[ 1.8429e-01,  9.7753e-02,  5.8055e-02,  ..., -8.2816e-02,\n",
      "          -1.3097e-01,  2.4475e-01],\n",
      "         [ 1.8536e-01,  9.7138e-02,  6.0830e-02,  ..., -1.2972e-01,\n",
      "          -3.1096e-02,  2.8740e-01],\n",
      "         [ 1.8516e-01,  9.7068e-02,  6.0915e-02,  ..., -1.3681e-01,\n",
      "          -3.2419e-02,  2.9291e-01],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02]],\n",
      "\n",
      "        [[ 1.8369e-01,  9.5775e-02,  6.2278e-02,  ..., -1.7294e-01,\n",
      "          -5.3587e-02,  3.1780e-01],\n",
      "         [ 1.8377e-01,  9.6069e-02,  6.1317e-02,  ..., -1.4559e-01,\n",
      "          -7.7798e-02,  2.9497e-01],\n",
      "         [ 1.8458e-01,  9.6249e-02,  6.1279e-02,  ..., -1.2496e-01,\n",
      "          -6.1523e-02,  2.8015e-01],\n",
      "         [ 1.8491e-01,  9.6041e-02,  6.2195e-02,  ..., -1.3985e-01,\n",
      "          -2.8913e-02,  2.9470e-01],\n",
      "         [ 1.8472e-01,  9.6045e-02,  6.2045e-02,  ..., -1.4090e-01,\n",
      "          -3.7052e-02,  2.9482e-01],\n",
      "         [ 1.8526e-01,  9.6720e-02,  6.0240e-02,  ..., -8.1285e-02,\n",
      "          -7.7276e-02,  2.4524e-01]],\n",
      "\n",
      "        [[ 1.8387e-01,  9.6251e-02,  6.0784e-02,  ..., -1.6243e-01,\n",
      "          -6.1463e-02,  3.0745e-01],\n",
      "         [ 1.8401e-01,  9.6764e-02,  5.9138e-02,  ..., -1.1523e-01,\n",
      "          -1.0371e-01,  2.6830e-01],\n",
      "         [ 1.8435e-01,  9.6284e-02,  6.0940e-02,  ..., -1.5404e-01,\n",
      "          -4.5166e-02,  3.0291e-01],\n",
      "         [ 1.8477e-01,  9.6773e-02,  5.9671e-02,  ..., -1.1060e-01,\n",
      "          -7.3280e-02,  2.6636e-01],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.8400e-01,  9.6956e-02,  6.1651e-02,  ..., -1.6253e-01,\n",
      "          -6.0149e-02,  3.1747e-01],\n",
      "         [ 1.8600e-01,  9.8714e-02,  5.7533e-02,  ..., -1.1798e-02,\n",
      "          -1.3603e-01,  1.9572e-01],\n",
      "         [ 1.8864e-01,  9.7546e-02,  6.2905e-02,  ..., -8.2362e-02,\n",
      "           6.0695e-02,  2.6332e-01],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02]],\n",
      "\n",
      "        [[ 1.8724e-01,  9.6485e-02,  6.0124e-02,  ..., -1.4839e-01,\n",
      "          -8.7049e-02,  3.0951e-01],\n",
      "         [ 1.9046e-01,  9.7480e-02,  5.9356e-02,  ..., -5.5076e-02,\n",
      "          -4.4060e-02,  2.3835e-01],\n",
      "         [ 1.9188e-01,  9.9505e-02,  5.9974e-02,  ...,  3.1485e-01,\n",
      "          -1.5317e-01,  2.0905e-01],\n",
      "         [ 1.8846e-01,  9.6190e-02,  6.2156e-02,  ..., -1.7415e-01,\n",
      "           2.2214e-04,  3.3336e-01],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02]],\n",
      "\n",
      "        [[ 1.8363e-01,  9.5633e-02,  6.4191e-02,  ..., -1.6576e-01,\n",
      "          -6.3540e-02,  3.1756e-01],\n",
      "         [ 1.8821e-01,  9.6435e-02,  6.4682e-02,  ..., -6.9201e-02,\n",
      "           3.6483e-02,  2.5128e-01],\n",
      "         [ 1.8679e-01,  9.6517e-02,  6.3446e-02,  ..., -7.3595e-02,\n",
      "          -2.2515e-02,  2.4945e-01],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02],\n",
      "         [ 2.8782e-03,  3.1115e-03, -4.2708e-03,  ...,  4.5094e-04,\n",
      "          -1.6188e-03,  1.0060e-02]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_substaions = []\n",
    "lengths = []\n",
    "for subset_node_embeddings_idx in sub_id_to_elem_id.values():\n",
    "    #print(subset_node_embeddings_idx)\n",
    "    subset_node_embeddings = node_embeddings[:, subset_node_embeddings_idx, :]\n",
    "    context_embedding = torch.mean(substation_embeddings, dim = 1, keepdim=True).expand(subset_node_embeddings.shape)\n",
    "    enriched_node_embedding = torch.cat((context_embedding, subset_node_embeddings), dim = 2)\n",
    "    \n",
    "    batch_substaions.append(enriched_node_embedding.squeeze(0))\n",
    "    lengths.append(len(subset_node_embeddings_idx))\n",
    "    #print(enriched_node_embedding.shape)\n",
    "\n",
    "padded = pad_sequence(batch_substaions, batch_first=True)#.shape\n",
    "\n",
    "adj_matrices = torch.zeros(padded.shape[0], padded.shape[1], padded.shape[1])\n",
    "for i, lentgh in enumerate(lengths):\n",
    "    adj_matrices[i, :lentgh, :lentgh] = 1\n",
    "\n",
    "#adj_matrices = adj_matrices.masked_fill(adj_matrices==0, -np.inf)\n",
    "adj_matrices\n",
    "#dec(padded)\n",
    "#lenghts   \n",
    "out = dec(padded, adj_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2530,  0.1137,  0.0658,  ..., -0.0908, -0.1273,  0.2096],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.1451, -0.0122,  0.2563],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.1530, -0.0138,  0.2625],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.2530,  0.1137,  0.0658,  ..., -0.1925, -0.0374,  0.2897],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.1616, -0.0650,  0.2645],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.1387, -0.0457,  0.2477],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.1559, -0.0080,  0.2637],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.1569, -0.0175,  0.2639],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.0894, -0.0631,  0.2087]],\n",
       "\n",
       "        [[ 0.2530,  0.1137,  0.0658,  ..., -0.1794, -0.0479,  0.2787],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.1259, -0.0961,  0.2354],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.1703, -0.0287,  0.2735],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.1212, -0.0606,  0.2330],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2530,  0.1137,  0.0658,  ..., -0.1849, -0.0458,  0.2903],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.0140, -0.1308,  0.1545],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.0967,  0.0955,  0.2283],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.2530,  0.1137,  0.0658,  ..., -0.1711, -0.0727,  0.2831],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.0668, -0.0210,  0.2027],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ...,  0.3527, -0.1413,  0.1702],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.2010,  0.0284,  0.3091],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.2530,  0.1137,  0.0658,  ..., -0.1891, -0.0463,  0.2892],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.0822,  0.0713,  0.2133],\n",
       "         [ 0.2530,  0.1137,  0.0658,  ..., -0.0863,  0.0030,  0.2120],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =F.softmax(adj_matrices + 1e-15, dim = 2)\n",
    "torch.sum(a, dim = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "        [[0.2112, 0.2112, 0.2112, 0.2112, 0.0777, 0.0777],\n",
       "         [0.2112, 0.2112, 0.2112, 0.2112, 0.0777, 0.0777],\n",
       "         [0.2112, 0.2112, 0.2112, 0.2112, 0.0777, 0.0777],\n",
       "         [0.2112, 0.2112, 0.2112, 0.2112, 0.0777, 0.0777],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "        [[0.1863, 0.1863, 0.1863, 0.1863, 0.1863, 0.0685],\n",
       "         [0.1863, 0.1863, 0.1863, 0.1863, 0.1863, 0.0685],\n",
       "         [0.1863, 0.1863, 0.1863, 0.1863, 0.1863, 0.0685],\n",
       "         [0.1863, 0.1863, 0.1863, 0.1863, 0.1863, 0.0685],\n",
       "         [0.1863, 0.1863, 0.1863, 0.1863, 0.1863, 0.0685],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "        [[0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "        [[0.2881, 0.2881, 0.1060, 0.1060, 0.1060, 0.1060],\n",
       "         [0.2881, 0.2881, 0.1060, 0.1060, 0.1060, 0.1060],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "        [[0.1863, 0.1863, 0.1863, 0.1863, 0.1863, 0.0685],\n",
       "         [0.1863, 0.1863, 0.1863, 0.1863, 0.1863, 0.0685],\n",
       "         [0.1863, 0.1863, 0.1863, 0.1863, 0.1863, 0.0685],\n",
       "         [0.1863, 0.1863, 0.1863, 0.1863, 0.1863, 0.0685],\n",
       "         [0.1863, 0.1863, 0.1863, 0.1863, 0.1863, 0.0685],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "        [[0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "        [[0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "        [[0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "        [[0.2112, 0.2112, 0.2112, 0.2112, 0.0777, 0.0777],\n",
       "         [0.2112, 0.2112, 0.2112, 0.2112, 0.0777, 0.0777],\n",
       "         [0.2112, 0.2112, 0.2112, 0.2112, 0.0777, 0.0777],\n",
       "         [0.2112, 0.2112, 0.2112, 0.2112, 0.0777, 0.0777],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "        [[0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.2437, 0.2437, 0.2437, 0.0896, 0.0896, 0.0896],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function GymEnv.__del__ at 0x7f8278b61170>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/rlib_grid/lib/python3.7/site-packages/grid2op/gym_compat/gymenv.py\", line 81, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/miniconda3/envs/rlib_grid/lib/python3.7/site-packages/grid2op/gym_compat/gymenv.py\", line 72, in close\n",
      "    self.action_space.close()\n",
      "AttributeError: 'CustomDiscreteActions' object has no attribute 'close'\n"
     ]
    }
   ],
   "source": [
    "a[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'num_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d8/8p2fsxqj3kl2z2ckh79w60c00000gn/T/ipykernel_76683/3024513993.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNodeModelNodeLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'num_features'"
     ]
    }
   ],
   "source": [
    "dec = NodeModelNodeLevel(num_features = 7, hidden_dim = 128, nheads = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.mean(out[:, [0,1,2], : ], 1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "target = [[[1,2,3], \n",
    "            [2,4,5,6]], [[1,2,3], [2,4,5,6], [2,4,6,7,8]]]#[0]\n",
    "max_length = max(len(row) for row in target)\n",
    "print(max_length)\n",
    "max_cols = max([len(row) for batch in target for row in batch])\n",
    "max_rows = max([len(batch) for batch in target])\n",
    "padded = [batch + [[0] * (max_cols)] * (max_rows - len(batch)) for batch in target]\n",
    "padded = torch.tensor([row + [0] * (max_cols - len(row)) for batch in padded for row in batch])\n",
    "padded = padded.view(-1, max_rows, max_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [2, 4, 5, 6]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd1a1f66bec4b7b5ad4da0007ab235e778c87d19785067c485d2ce55023da22c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('rlib_grid': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
